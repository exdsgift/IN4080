{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN4080: obligatory assignment 4\n",
    " \n",
    "The final mandatory assignment for IN4080 consists of two parts. The first is about the development of dialogue systems, and the second about machine translation.\n",
    "You are required to get at least 12/20 points to pass. \n",
    "\n",
    "- We assume that you have read and are familiar with IFI’s requirements and guidelines for mandatory assignments, see [here](https://www.uio.no/english/studies/examinations/compulsory-activities/mn-ifi-mandatory.html) and [here](https://www.uio.no/english/studies/examinations/compulsory-activities/mn-ifi-guidelines.html).\n",
    "- This is an individual assignment. You should not deliver joint submissions. \n",
    "- You may redeliver in Devilry before the deadline (__Sunday, November 3 at 23:59__).\n",
    "- Only the last delivery will be read! If you deliver more than one file, put them into a zip-archive. You don't have to include in your delivery the data files already provided for this assignment. \n",
    "- Name your submission _your\\_username\\_in4080\\_mandatory\\_4_\n",
    "\n",
    "Part 1 should be done on your local computer, as it relies on a speech interface that will not work on remote machines. For Part 2, using _Fox_ is preferable, at least for the fine-tuning task.\n",
    "\n",
    "You should deliver a completed version of this Jupyter notebook, containing both your code and explanations about the steps you followed. We want to stress that simply submitting code is __not__ by itself sufficient to complete the assignment - we expect the notebook to also contain explanations of what you have implemented, along with motivations for the choices you made along the way. Preferably use whole sentences, and mathematical formulas if necessary. Explaining in your own words (using concepts we have covered through in the lectures) what you have implemented and reflecting on your solution is an important part of the learning process - take it seriously!\n",
    "\n",
    "Regarding the use of LLMs (ChatGPT or similar): you are allowed to use them as 'sparring partner', for instance to clarify something you have not understood. However, you are __not__ allowed to use them to generate solutions (either in part or in full) to the assignment tasks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 : Dialogue systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective in this part is to build a spoken conversational interface for a (simulated) elevator. \n",
    "\n",
    "### Basic setup\n",
    "\n",
    "First, let's make sure that we have all the necessary Python modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install ipywidgets pyaudio openai-whisper pyttsx3 setfit spacy jellyfish\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for the simulated elevator is provided below. The elevator is displayed using simple widgets (where the current floor is shown in green). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "import time, random, string, threading\n",
    "from typing import List, Tuple, Dict, Set\n",
    "\n",
    "class BasicElevator:\n",
    "    \"\"\"Elevator simulated using a GUI\"\"\"\n",
    "    \n",
    "    def __init__(self, start_floor:int =1, nb_floors=10):\n",
    "        \"\"\"Initialised a new elevator, placed on the first floor\"\"\"\n",
    "        \n",
    "        # Current floor of the elevator\n",
    "        self.cur_floor: int = start_floor\n",
    "\n",
    "        # (Possibly empty) list of next floor stops to reach \n",
    "        self.next_stops : List[int] = []\n",
    "        \n",
    "        # Building the basic GUI showing the elevator\n",
    "        display(self._build_gui(nb_floors))\n",
    "\n",
    "        # Starts the thread executing the movements\n",
    "        thread = threading.Thread(target=self.elevator_move_thread)\n",
    "        thread.start()     \n",
    "            \n",
    "    def move_to_floor(self, floor_number : int):\n",
    "        \"\"\"Move to a given floor (by adding it to a stack of floors to reach)\"\"\"\n",
    "        \n",
    "        if floor_number < 1 or floor_number > len(self.floors):\n",
    "            raise RuntimeError(\"Floor number must be between 1 and %i\"%len(self.floors))\n",
    "\n",
    "        self.next_stops.append(floor_number)\n",
    "        \n",
    "        \n",
    "    def stop(self):\n",
    "        \"\"\"Stops all movements of the elevator\"\"\"\n",
    "        self.next_stops.clear()\n",
    "\n",
    "    def _build_gui(self, nb_floors):\n",
    "        \"\"\"Creates the GUI for the elevator, with a status label and a visual representation\n",
    "        of the floors, where the current floor is indicated in green.\"\"\"\n",
    "\n",
    "        # Displaying the current status of the elevator (still or going up or down)\n",
    "        status_label = widgets.HTML(\"<b>Status</b>: \")\n",
    "        self.status = widgets.Label(\"Still\")\n",
    "        status_box = widgets.HBox([status_label, self.status])\n",
    "\n",
    "        # Displaying the floors on a vertical axis\n",
    "        self.floors = []\n",
    "        floor_layout = widgets.Layout(width='50px', height='30px', border='2px solid black',justify_content=\"center\")\n",
    "        for i in range(1, nb_floors+1):\n",
    "            floor = widgets.Label(value=str(i), layout=floor_layout)\n",
    "            floor.style = {\"background\":(\"white\" if i!=self.cur_floor else \"lightgreen\")}\n",
    "            self.floors.append(floor)\n",
    "\n",
    "        # Create a vertical box container to hold the boxes\n",
    "        vbox = widgets.VBox([status_box] + self.floors[::-1])\n",
    "        return vbox\n",
    "    \n",
    "\n",
    "    def elevator_move_thread(self, speed=1.0, latency=0.1):\n",
    "        \"\"\"Trigger a movement of the elevator if the list of next stops is not \n",
    "        empty. The movement continues until all goals are reached.\"\"\"\n",
    "\n",
    "        while True:\n",
    "            while self.next_stops:\n",
    "                if self.cur_floor == self.next_stops[0]:\n",
    "                    del self.next_stops[0]\n",
    "                    continue\n",
    "                if self.cur_floor < self.next_stops[0]:\n",
    "                    next_floor = self.cur_floor+1\n",
    "                    self.status.value = \"UP\"\n",
    "                elif self.cur_floor > self.next_stops[0]:\n",
    "                    next_floor = self.cur_floor-1\n",
    "                    self.status.value = \"DOWN\"\n",
    "                time.sleep(speed)   \n",
    "                self.floors[self.cur_floor-1].style.background = \"white\"\n",
    "                self.floors[next_floor-1].style.background = \"lightgreen\"\n",
    "                self.cur_floor = next_floor\n",
    "            self.status.value = \"Still\"\n",
    "            \n",
    "            # Wait loop (until we have a goal in self.next_stops)\n",
    "            time.sleep(latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elevator can be easily controlled through the functions `move_to_floor` and `stop`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954337384c6744b08188bde5ff7917f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HTML(value='<b>Status</b>: '), Label(value='Still'))), Label(value='10', layout=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "elevator = BasicElevator()\n",
    "elevator.move_to_floor(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now make our elevator controllable through a speech interface instead of using function calls.\n",
    "\n",
    "## Speech interface\n",
    "\n",
    "First, make sure that you have installed `pyaudio` (for audio processing), `whisper` (for speech recognition), and `pyttsx3` (for speech synthesis).\n",
    "\n",
    "The `TalkingElevator` class below extends the basic simulated elevator with speech input and output. \n",
    "\n",
    "Upon clicking on the recording button, speech is recorded from the user's microphone, and continues until the stop button is clicked. The speech recognition engine `Whisper` from OpenAI is then employed to transcribe the spoken input (either on GPU, if you have a GPU on your machine, or on CPU). The transcription result is then sent to the `process_input` function, which is responsible for determining the system response. \n",
    "\n",
    "We are going to focus on implementing this `process_input` method. Note this system reaction to new user inputs may comprise both verbal responses (to be uttered by the system through the `_say_to_user` method) and physical actions (through the `move_to_floor` and `stop` methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'whisper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Tuple, Dict, Set\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwhisper\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mpyaudio\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mpyttsx3\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m      6\u001b[0m warnings\u001b[38;5;241m.\u001b[39msimplefilter(action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'whisper'"
     ]
    }
   ],
   "source": [
    "import threading, time\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, Set\n",
    "import whisper, pyaudio, pyttsx3\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "class TalkingElevator(BasicElevator):\n",
    "    \"\"\"Extension of the simulated elevator with a speech interface\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"Loading TTS and ASR models\", end=\"...\", flush=True)\n",
    "        self.tts_engine = pyttsx3.init()  \n",
    "        self.asr_engine = whisper.load_model(\"small.en\")\n",
    "        print(\"Done\")\n",
    "        \n",
    "        # Initializing the GUI\n",
    "        BasicElevator.__init__(self)\n",
    "\n",
    "        # Starts the dialogue\n",
    "        self.dialogue_history = []\n",
    "        self._say_to_user(\"Hi, what can I do for you today?\")\n",
    "    \n",
    "\n",
    "    def process_input(self, user_input: str, conf_score:float=1.0):\n",
    "        \"\"\"Processes the (transcribed) user input, and respond appropriately \n",
    "        (through a verbal response and possibly also an action, such as moving floors)\"\"\"\n",
    "\n",
    "        self._add_to_dialogue_history(user_input, speaker=\"user\", conf_score=conf_score)\n",
    "\n",
    "        # Dummy response. Should be replaced by the actual dialogue behaviour\n",
    "        self._say_to_user(\"Sorry, I don't understand you, pal\")\n",
    "\n",
    "    \n",
    "    def _say_to_user(self, system_response: str):\n",
    "        \"\"\"Say something back to the user, and add the dialogue turn to the history. The \n",
    "        synthesis is done using the pyttsx3 library.\"\"\"\n",
    "\n",
    "        self._add_to_dialogue_history(system_response, speaker=\"elevator\")\n",
    "        \n",
    "        # Stopping current TTS if one is active\n",
    "        try:\n",
    "            self.tts_engine.endLoop()\n",
    "        except:\n",
    "            pass\n",
    "        self.tts_engine.say(system_response)\n",
    "        self.tts_engine.runAndWait()\n",
    "\n",
    "\n",
    "    def _add_to_dialogue_history(self, turn:str , speaker:str, conf_score:float=1.0):\n",
    "        \"\"\"Adds a new (user or system) turn to the dialogue history list, and displays it\n",
    "         on the chat window displaying the turns\"\"\"\n",
    "\n",
    "        self.dialogue_history.append({\"speaker\":speaker, \"text\":turn, \n",
    "                                      \"conf_score\":conf_score, \"timesamp\":time.time()})\n",
    "        \n",
    "        self.history_area.value += \"&nbsp;<strong>%s</strong>:  %s\"%(speaker.title(), turn)\n",
    "        if conf_score < 1.0:\n",
    "            self.history_area.value += \" (%.2f)\"%(conf_score)\n",
    "        self.history_area.value += \"<br>\"\n",
    "   \n",
    "   \n",
    "    def _build_gui(self, nb_floors):\n",
    "        \"\"\"GUI for the Talking elevator, comprising (beyond the simulated elevator from \n",
    "        BasicElevator) a chat window showing the dialogue turns, and buttons to record\n",
    "        the user input. \n",
    "        The user should first click on the record button, then on stop when they have finished.\n",
    "        Once the stop button is clicked, the audio is transcribed by Whisper, and finally \n",
    "        forwarded to the process_input function.\"\"\"\n",
    "\n",
    "        core_gui = BasicElevator._build_gui(self, nb_floors)\n",
    "\n",
    "        self.frames = []\n",
    "        self.recording = False\n",
    "\n",
    "        def record(chunk_size=1024):\n",
    "            \"\"\"Record audio chunks to a buffer.\"\"\"\n",
    "            self.recording = True\n",
    "            p = pyaudio.PyAudio()\n",
    "            stream = p.open(format=pyaudio.paInt16, channels=1, rate=16000, input=True, \n",
    "                            frames_per_buffer=chunk_size)\n",
    "            while self.recording:\n",
    "                self.frames.append(stream.read(chunk_size))\n",
    "            stream.close()\n",
    "\n",
    "        def on_record_button_clicked(b):\n",
    "            \"Starts the recording\"\n",
    "            record_button.disabled=True\n",
    "            stop_button.disabled=False\n",
    "            self.frames = []  # Clear previous recordings\n",
    "            thread = threading.Thread(target=record)\n",
    "            thread.start()\n",
    "\n",
    "        def on_stop_button_clicked(b):\n",
    "            \"stops the recording, runs Whisper, and forward the result to process_input\"\n",
    "            self.recording = False\n",
    "            record_button.disabled=False\n",
    "            stop_button.disabled=True\n",
    "            audio_data = np.frombuffer(b\"\".join(self.frames), np.int16).astype(np.float32) * (1 / 32768.0)\n",
    "            output = self.asr_engine.transcribe(audio_data)\n",
    "\n",
    "            # We define the confidence score based on the log-probabilities\n",
    "            conf_score = np.exp(np.mean([segment[\"avg_logprob\"] for segment in output[\"segments\"]]))\n",
    "            # (and we push those up a bit, as the Whisper scores seem too low)\n",
    "            conf_score = min(1, conf_score*1.2)\n",
    "\n",
    "            # Finally, we process the input\n",
    "            self.process_input(output[\"text\"], conf_score)\n",
    "\n",
    "        # The record and stop buttons\n",
    "        record_button = widgets.Button(icon=\"microphone\")\n",
    "        stop_button = widgets.Button(icon=\"stop\", disabled=True)\n",
    "        record_button.on_click(on_record_button_clicked)\n",
    "        stop_button.on_click(on_stop_button_clicked)\n",
    "        \n",
    "        # The chat area\n",
    "        self.history_area = widgets.HTML(layout=widgets.Layout(width=\"600px\", height=\"300px\", \n",
    "                                                               border='1px solid black', overflow='scroll'))\n",
    "\n",
    "        # The right side of the GUI\n",
    "        right_side = widgets.VBox([widgets.Label(\"\"), self.history_area, widgets.HBox([record_button, stop_button])])\n",
    "        \n",
    "        extended_gui = widgets.HBox([core_gui, right_side])\n",
    "        return extended_gui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give it a try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elevator = TalkingElevator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hope that the speech recognition and speech synthesis will work correctly -- if it isn't the case, do let us know ! (audio processing in Python can be quite tricky and will work differently from OS to OS). [^1]\n",
    "\n",
    "[^1]: If you are running on Linux and the TTS is not working, install the following packages on your machine: `sudo apt update && sudo apt install espeak ffmpeg libespeak1`\n",
    "\n",
    "**Note**: The current implementation reloads the TTS and ASR models every time, which means that you may run into a \"CUDA: out of memory\" error if you reinitialise the `TalkingElevator` many times. If this happens, simply restart the Python kernel, which will clear the memory on both the CPU and the GPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Intent recognition\n",
    "\n",
    "We wish our talking elevator to support the following functionalities:\n",
    " \n",
    "- If the user express a wish to go to floor $X$ (where $X$ is an integer value between 1 and 10), the elevator should go to that floor. The interface should allow for several ways to express a given intent, such as \"_Please go to the $X$-th floor_\" or \"_Floor $X$, please_\".\n",
    "- The user requests can also be relative, for instance \"_Go one floor up_\".\n",
    "- The elevator should provide _grounding_ feedback to the user. For instance, it should respond \"_Ok, going to the $X$-th floor_\" after a user request to move to $X$.  \n",
    "- The elevator should handle misunderstandings and uncertainties, e.g. by requesting the user to repeat, or asking the user to confirm if the intent is uncertain (say, when its confidence score is lower than 0.5). \n",
    "- The elevator should also allow the user to ask where the office of a given employee is located. For instance, the user could ask \"_where is Erik Velldal's office?_\", and the elevator would provide a response such as \"_The office of Erik Velldal is on the 4th floor. Do you wish to go there?_\".  We provide you with the office numbers of a small set of IFI employees in the `OFFICES` dictionary (see below).\n",
    "- The elevator should also be able to inform the user about the current floor (such as replying to \"_Which floor are we on?_\" or \"_Are we on the 5th floor?_\"). \n",
    "- Finally, if the user asks the elevator to stop (or if the user says \"_no_\" after a grounding feedback \"_Ok, going to floor $X$._\"), the elevator should stop, and ask for clarification regarding the actual user intent. \n",
    "\n",
    "To implement this conversational behaviour, we will rely on a classical NLU-based approach in which we will recognise the user _intent_, and then determine a response based on the recognised intent(s). \n",
    "\n",
    "__Task 1.1__ (1 point): You first need to define a list of user intents that cover the kinds of user inputs you expect to observe in this talking elevator, such as `RequestMoveToFloor` or `Confirm`. This is a design question, and there is no obvious right or wrong answer. Define below the intents you want to cover, along with an explanation and a few examples of user inputs for each.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Provide here the list of intent classes you have defined, together with an explanation and a few examples -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_intents = [\n",
    "    \"Please go to the 3rd floor.\",\n",
    "    \"Floor 7, please.\",\n",
    "    \"I want to go to floor 5.\",\n",
    "    \"Go one floor up.\",\n",
    "    \"Take me to the 2nd floor.\",\n",
    "    \"What floor is this?\",\n",
    "    \"Can you tell me where Anna's office is?\",\n",
    "    \"I need to find Lars' office.\",\n",
    "    \"Did you mean the 5th floor?\",\n",
    "    \"Are you asking to go to the 7th floor?\",\n",
    "    \"Stop!\",\n",
    "    \"No, I changed my mind.\",\n",
    "    \"Wait, I don’t want to go to that floor.\",\n",
    "    \"I’m not sure what you meant. Can you say that again?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 1.2__ (1 points): We wish to build a classifier any user input to a probability distribution over those intents, and start by creating a small, synthetic training set. Make a list of about 100 user utterances, each labelled with an intent defined above. You can \"make up\" those utterances yourself, or ask someone else to come with alternative formulations if you lack inspiration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_utterances = [\n",
    "    (\"Please go to the 3rd floor.\", \"RequestMoveToFloor\"),\n",
    "    (\"Floor 7, please.\", \"RequestMoveToFloor\"),\n",
    "    (\"I want to go to the 5th floor.\", \"RequestMoveToFloor\"),\n",
    "    (\"Can you take me to the 2nd floor?\", \"RequestMoveToFloor\"),\n",
    "    (\"Go one floor up.\", \"RequestMoveToFloor\"),\n",
    "    (\"Take me to floor 8.\", \"RequestMoveToFloor\"),\n",
    "    (\"I need to go to the 1st floor.\", \"RequestMoveToFloor\"),\n",
    "    (\"Go down to the 4th floor.\", \"RequestMoveToFloor\"),\n",
    "    (\"I'd like to reach the 10th floor.\", \"RequestMoveToFloor\"),\n",
    "    (\"Could you bring me to the 6th floor?\", \"RequestMoveToFloor\"),\n",
    "    (\"Let's go to floor 9!\", \"RequestMoveToFloor\"),\n",
    "    (\"One floor down, please.\", \"RequestMoveToFloor\"),\n",
    "    (\"I want to go to the roof.\", \"RequestMoveToFloor\"),\n",
    "    (\"Can you take me to the basement?\", \"RequestMoveToFloor\"),\n",
    "    (\"Head to floor number 3.\", \"RequestMoveToFloor\"),\n",
    "    (\"Up to the 2nd floor, please.\", \"RequestMoveToFloor\"),\n",
    "    (\"Get me to the 5th floor, please.\", \"RequestMoveToFloor\"),\n",
    "    (\"I need to go to the 4th floor now.\", \"RequestMoveToFloor\"),\n",
    "    (\"Let's go to the ground floor.\", \"RequestMoveToFloor\"),\n",
    "    (\"Please take me to floor 1.\", \"RequestMoveToFloor\"),\n",
    "    \n",
    "    (\"Ok, going to the 4th floor.\", \"GroundingFeedback\"),\n",
    "    (\"Sure, I will take you to the 2nd floor now.\", \"GroundingFeedback\"),\n",
    "    (\"Got it, heading to the 5th floor.\", \"GroundingFeedback\"),\n",
    "    (\"Alright, going to floor 7.\", \"GroundingFeedback\"),\n",
    "    (\"Okay, I’ll take you to the 3rd floor.\", \"GroundingFeedback\"),\n",
    "    \n",
    "    (\"Which floor are we on?\", \"RequestCurrentFloor\"),\n",
    "    (\"Are we on the 3rd floor?\", \"RequestCurrentFloor\"),\n",
    "    (\"What floor is this?\", \"RequestCurrentFloor\"),\n",
    "    (\"Can you tell me what floor we're currently on?\", \"RequestCurrentFloor\"),\n",
    "    (\"Please let me know the current floor.\", \"RequestCurrentFloor\"),\n",
    "    (\"Am I on the 6th floor right now?\", \"RequestCurrentFloor\"),\n",
    "    \n",
    "    (\"Where is Erik Velldal's office?\", \"RequestEmployeeLocation\"),\n",
    "    (\"Can you tell me where Anna's office is?\", \"RequestEmployeeLocation\"),\n",
    "    (\"I need to find Lars' office.\", \"RequestEmployeeLocation\"),\n",
    "    (\"Where can I find the office of Erik Velldal?\", \"RequestEmployeeLocation\"),\n",
    "    (\"Could you point me to Anna's office?\", \"RequestEmployeeLocation\"),\n",
    "    (\"I am looking for Lars' office, where is it?\", \"RequestEmployeeLocation\"),\n",
    "    (\"Tell me where Erik Velldal works.\", \"RequestEmployeeLocation\"),\n",
    "    (\"Where can I find the office of Anna?\", \"RequestEmployeeLocation\"),\n",
    "    \n",
    "    (\"Did you mean the 5th floor?\", \"Confirm\"),\n",
    "    (\"Are you asking to go to the 7th floor?\", \"Confirm\"),\n",
    "    (\"Is it correct that you want to go to the 2nd floor?\", \"Confirm\"),\n",
    "    (\"Are you sure you want to go to floor 9?\", \"Confirm\"),\n",
    "    (\"Did you say the 10th floor?\", \"Confirm\"),\n",
    "    \n",
    "    (\"Stop!\", \"RequestStop\"),\n",
    "    (\"No, I changed my mind.\", \"RequestStop\"),\n",
    "    (\"Wait, I don’t want to go to that floor.\", \"RequestStop\"),\n",
    "    (\"Please stop here.\", \"RequestStop\"),\n",
    "    (\"I want to get off now!\", \"RequestStop\"),\n",
    "\n",
    "    (\"I didn't catch that, could you repeat?\", \"HandleMisunderstanding\"),\n",
    "    (\"I’m not sure what you meant. Can you say that again?\", \"HandleMisunderstanding\"),\n",
    "    (\"Can you clarify that?\", \"HandleMisunderstanding\"),\n",
    "    (\"I don't understand, please repeat.\", \"HandleMisunderstanding\"),\n",
    "    (\"What did you say? I didn't hear you.\", \"HandleMisunderstanding\"),\n",
    "    \n",
    "    # Additional utterances for each intent\n",
    "    (\"Go to the 8th floor now.\", \"RequestMoveToFloor\"),\n",
    "    (\"Can we go to the 4th floor?\", \"RequestMoveToFloor\"),\n",
    "    (\"Elevator, go up one floor.\", \"RequestMoveToFloor\"),\n",
    "    \n",
    "    (\"Understood, heading to the 9th floor.\", \"GroundingFeedback\"),\n",
    "    (\"Okay, we will go to floor 1.\", \"GroundingFeedback\"),\n",
    "    \n",
    "    (\"What floor are we on right now?\", \"RequestCurrentFloor\"),\n",
    "    (\"Could you tell me the current level?\", \"RequestCurrentFloor\"),\n",
    "    \n",
    "    (\"Where's Lars' office located?\", \"RequestEmployeeLocation\"),\n",
    "    (\"I need to know where Erik's office is.\", \"RequestEmployeeLocation\"),\n",
    "    \n",
    "    (\"You meant the 8th floor, right?\", \"Confirm\"),\n",
    "    (\"So, you want to go to the 3rd floor?\", \"Confirm\"),\n",
    "    \n",
    "    (\"Please halt the elevator.\", \"RequestStop\"),\n",
    "    (\"Stop going up!\", \"RequestStop\"),\n",
    "    \n",
    "    (\"I don't get what you said, please repeat it.\", \"HandleMisunderstanding\"),\n",
    "    (\"Sorry, I didn’t catch that, could you say it again?\", \"HandleMisunderstanding\"),\n",
    "\n",
    "    # Additional examples for diverse utterances\n",
    "    (\"Bring me to the 5th floor.\", \"RequestMoveToFloor\"),\n",
    "    (\"Let's stop here for a moment.\", \"RequestStop\"),\n",
    "    (\"Can you stop the elevator, please?\", \"RequestStop\"),\n",
    "    (\"Where is Anna's office on campus?\", \"RequestEmployeeLocation\"),\n",
    "    (\"Confirm if I should go to the 3rd floor.\", \"Confirm\"),\n",
    "    (\"Where are we now?\", \"RequestCurrentFloor\"),\n",
    "    (\"Go down to the first floor, please.\", \"RequestMoveToFloor\"),\n",
    "    (\"I want to go up to the 6th floor.\", \"RequestMoveToFloor\"),\n",
    "    (\"Show me where Erik's office is.\", \"RequestEmployeeLocation\"),\n",
    "    (\"Let me know which floor this is.\", \"RequestCurrentFloor\"),\n",
    "    (\"I'm trying to find Lars' office.\", \"RequestEmployeeLocation\"),\n",
    "    (\"Could you stop for a moment?\", \"RequestStop\"),\n",
    "    (\"I need to go to the roof, please.\", \"RequestMoveToFloor\"),\n",
    "    (\"I want to get off on the ground floor.\", \"RequestMoveToFloor\"),\n",
    "    (\"What level are we on?\", \"RequestCurrentFloor\"),\n",
    "    (\"I want to go to the 10th floor now.\", \"RequestMoveToFloor\"),\n",
    "    (\"Could you take me down one floor?\", \"RequestMoveToFloor\"),\n",
    "    (\"Stop at the next floor.\", \"RequestStop\"),\n",
    "    (\"I need to confirm my floor.\", \"Confirm\"),\n",
    "    (\"Do you mean floor 2?\", \"Confirm\"),\n",
    "    (\"I want to go to floor 7, is that correct?\", \"Confirm\"),\n",
    "    (\"Please repeat that, I didn’t hear you.\", \"HandleMisunderstanding\"),\n",
    "    (\"What floor are we supposed to go to?\", \"HandleMisunderstanding\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train an intent classifier based on the labelled utterances you have defined. To do so, we will rely on the [SetFit](https://huggingface.co/docs/setfit/index) library, which allows one to easily train a text classification model from few examples by fine-tuning a sentence-transformer model (like the ones we used in oblig 2 and 3). Make sure that the `setfit` library is installed (`pip install setfit`).\n",
    "\n",
    "Read the [Setfit quickstart guide](https://huggingface.co/docs/setfit/quickstart) to find out how to use the library.\n",
    "\n",
    "__Task 1.3__ (2 points): Implement the `__init__`, `train` and `get_intent_distrib` methods of the `IntentClassifier` class below. The classifier should rely on a `Setfit` model trained on the labelled utterances you have already defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import setfit, datasets\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "class IntentClassifier:\n",
    "\n",
    "    def __init__(self, model_name=\"sentence-transformers/paraphrase-mpnet-base-v2\"):\n",
    "        \"\"\"Initialises the setfit model that will be used for the intent recognition\"\"\"\n",
    "        self.model = setfit.SetFitModel.from_pretrained(model_name)\n",
    "    \n",
    "    def train(self, labelled_utterances: List[Tuple[str,str]]):\n",
    "        \"\"\"Trains the setfit model on the labelled utterances\"\"\"\n",
    "        train_data = datasets.Dataset.from_list([{\"text\":utt, \"label\":label} \n",
    "                                                 for utt,label in labelled_utterances])\n",
    "        self.model.fit(train_data)\n",
    "        \n",
    "    def get_intent_distrib(self, utterance:str):\n",
    "        \"\"\"Applies the trained model on a new utterance. The method should return a\n",
    "        dictionary that maps each possible intent category to a probability.\"\"\"\n",
    "\n",
    "        probabilities = self.model.predict([utterance], return_proba=True)\n",
    "        intent_distribution = {label: prob for label, prob in zip(self.model.config.id2label, probabilities[0])}\n",
    "        \n",
    "        return intent_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = IntentClassifier()\n",
    "classifier.train(labelled_utterances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't have any test data, we cannot really conduct an evaluation of the classification performance, but this step would be of course strongly adviced when developing a real system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slot filling\n",
    "\n",
    "In addition to the intents themselves, we also wish to detect some slots, such as floor numbers or person names. For this step, we will not use a data-driven model, but rather rely on an old-fashioned, rule-based approach:\n",
    "- For floor numbers, we will rely on string matching (with regular expressions or basic string search) that detect patterns such as \"X floor\" (where X is [first,second, third, fourth, fifth, sixth, seventh, eighth, ninth, tenth]) or \"floor X\" (where X is between 1 and 10).\n",
    "- For person names, we have a predefined list of person names to detect (employees at IFI), and we should simply search for their occurrence in the user input. The simplest implementation is to just for look for exact occurrences. However, since speech recognition will often struggle to recognize foreign person names, an even better approach would be to search for names that are phonetically close (you can use the `jellyfish` library for this).\n",
    "\n",
    "The results of the slot filling should be a dictionary mapping slot names to a canonical form of the slot value. For instance, if the utterance contains the expression \"ninth floor\", the resulting slot dictionary should be `{\"floor_number\":9}`. Similarly, the `employee_name` slot should be a name present in `OFFICES` dictionary. \n",
    "\n",
    "__Task 1.4__ (2 points): Implement the method `fill_slots` that will detect the occurrence of those slots in the user input.<br>\n",
    "(+ 1 bonus point if you implement a fuzzy matching strategy to find person names that are phonetically close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Floor numbers for a subset of the IFI employees\n",
    "# OFFICES = {'Adín Ramírez Rivera': 4, 'Andreas Austeng': 4, 'Anne H Schistad Solberg': 4, \n",
    "#            'Arild Torolv Søetorp Waaler': 9, 'Audun Jøsang': 9, 'Birthe Soppe': 4, 'Carsten Griwodz': 4,\n",
    "#            'Dag Sjøberg': 9, 'Dag Trygve Eckhoff Wisland': 5, 'Einar Broch Johnsen': 8, \n",
    "#            'Eric Bartley Jul': 10, 'Erik Velldal': 4, 'Henrik Skaug Sætra': 7, 'Ingrid Chieh Yu': 8,\n",
    "#            'Jørn Anders Braa': 6, 'Kristin Bråthen': 4, 'Kyrre Glette': 4, 'Lars Groth': 6, \n",
    "#            'Lilja Øvrelid': 4, 'Maja Van Der Velden': 7, 'Martin Giese': 9, 'Michael Welzl': 5, \n",
    "#            'Miria Grisot': 6, 'Nils Gruschka': 9, 'Olaf Owe': 9, 'Ole Christian Lingjærde': 4, \n",
    "#            'Ole Hanseth': 6, 'Paulo Ferreira': 10, 'Philipp Dominik Häfliger': 5, 'Philipp Häfliger': 5, \n",
    "#            'Roman Vitenberg': 4, 'Silvia Lizeth Tapia Tarifa': 8, 'Stephan Oepen': 4, \n",
    "#            'Sundeep Sahay': 6, 'Thomas Peter Plagemann': 4, 'Tone Bratteteig': 7, 'Torbjørn Rognes': 8, \n",
    "#            'Truls Erikson': 6, 'Viktoria Stray': 10, 'Yngvar Berg': 5, 'Yves Scherrer': 4, \n",
    "#            'Özgü Mira Alay-Erduran': 4}\n",
    "\n",
    "# def fill_slots(user_input:str) -> Dict[str,str]:\n",
    "#     \"\"\"Extracts the set of slots detected in the user inputs. More precisely, the method\n",
    "#     should detect both floor numbers and person names, and return a dictionary mapping slot \n",
    "#     names (in this case either `floor_number` or `employee_name`) to its corresponding\n",
    "#     value, in canonical form (integer for the floor number, string for the employee name)\"\"\"\n",
    "\n",
    "#     raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jellyfish\n",
    "from typing import Dict, Optional\n",
    "\n",
    "# Floor numbers for a subset of the IFI employees\n",
    "OFFICES = {\n",
    "    'Adín Ramírez Rivera': 4, 'Andreas Austeng': 4, 'Anne H Schistad Solberg': 4, \n",
    "    'Arild Torolv Søetorp Waaler': 9, 'Audun Jøsang': 9, 'Birthe Soppe': 4, \n",
    "    'Carsten Griwodz': 4, 'Dag Sjøberg': 9, 'Dag Trygve Eckhoff Wisland': 5, \n",
    "    'Einar Broch Johnsen': 8, 'Eric Bartley Jul': 10, 'Erik Velldal': 4, \n",
    "    'Henrik Skaug Sætra': 7, 'Ingrid Chieh Yu': 8, 'Jørn Anders Braa': 6, \n",
    "    'Kristin Bråthen': 4, 'Kyrre Glette': 4, 'Lars Groth': 6, 'Lilja Øvrelid': 4, \n",
    "    'Maja Van Der Velden': 7, 'Martin Giese': 9, 'Michael Welzl': 5, \n",
    "    'Miria Grisot': 6, 'Nils Gruschka': 9, 'Olaf Owe': 9, \n",
    "    'Ole Christian Lingjærde': 4, 'Ole Hanseth': 6, 'Paulo Ferreira': 10, \n",
    "    'Philipp Dominik Häfliger': 5, 'Philipp Häfliger': 5, 'Roman Vitenberg': 4, \n",
    "    'Silvia Lizeth Tapia Tarifa': 8, 'Stephan Oepen': 4, 'Sundeep Sahay': 6, \n",
    "    'Thomas Peter Plagemann': 4, 'Tone Bratteteig': 7, 'Torbjørn Rognes': 8, \n",
    "    'Truls Erikson': 6, 'Viktoria Stray': 10, 'Yngvar Berg': 5, \n",
    "    'Yves Scherrer': 4, 'Özgü Mira Alay-Erduran': 4\n",
    "}\n",
    "\n",
    "def fill_slots(user_input: str) -> Dict[str, Optional[str]]:\n",
    "    \"\"\"Extracts the set of slots detected in the user inputs. Detects both floor numbers \n",
    "    and person names, returning a dictionary mapping slot names to their corresponding values.\"\"\"\n",
    "    \n",
    "    slots = {}\n",
    "\n",
    "    floor_patterns = [\n",
    "        r\"(?i)(first|second|third|fourth|fifth|sixth|seventh|eighth|ninth|tenth) floor\",  # Words\n",
    "        r\"(?i)floor (1|2|3|4|5|6|7|8|9|10)\"  # Numbers\n",
    "    ]\n",
    "\n",
    "    for pattern in floor_patterns:\n",
    "        match = re.search(pattern, user_input)\n",
    "        if match:\n",
    "            # Handle the word-based pattern\n",
    "            if match.group(0).startswith(('first', 'second', 'third', 'fourth', 'fifth', \n",
    "                                          'sixth', 'seventh', 'eighth', 'ninth', 'tenth')):\n",
    "                floor_number = ['first', 'second', 'third', 'fourth', 'fifth', \n",
    "                                'sixth', 'seventh', 'eighth', 'ninth', 'tenth'].index(match.group(0).split()[0]) + 1\n",
    "            else:\n",
    "                floor_number = int(match.group(0).split()[1])\n",
    "            slots['floor_number'] = floor_number\n",
    "            break\n",
    "\n",
    "    for name in OFFICES.keys():\n",
    "        if name.lower() in user_input.lower():\n",
    "            slots['employee_name'] = name\n",
    "            break\n",
    "\n",
    "    if 'employee_name' not in slots:\n",
    "        for name in OFFICES.keys():\n",
    "            if jellyfish.jaro_winkler_similarity(name.lower(), user_input.lower()) > 0.8:\n",
    "                slots['employee_name'] = name\n",
    "                break\n",
    "\n",
    "    return slots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response selection\n",
    "\n",
    "The next step is to implement the response selection mechanism. The response will depend on various factors:\n",
    "- the inferred user intents from the user utterance\n",
    "- the detected slot values in the user utterance (if any)\n",
    "- the current floor\n",
    "- the list of next floor stops that are yet to be reached\n",
    "- the dialogue history (as a list of dialogue turns).\n",
    "\n",
    "The response may consist of verbal responses (enacted by calls to `_say_to_user`) but also physical actions, represented by calls to either `move_to_floor` or `stop`. \n",
    "\n",
    "__Task 1.5__ (3 points): Implement the method `_respond`, which is responsible for selecting and executing those responses. The responses should satisfy the aforementioned conversational criteria (provide grounding feedback, use confirmations and clarification requests etc.). This method will consist in practice of many _if...then...else_ blocks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _respond(self, intent_distrib: Dict[str, float], slots: Dict[str,str]) :\n",
    "#     \"\"\"Given a probability distribution over possible intents, an a (possibly empty) list\n",
    "#     of detected slots in the user input, decide how to react. The method should lead\n",
    "#     to calls to both physical actions (move_to_floor, stop) and dialogue responses \n",
    "#     (via _say_to_user).\"\"\"\n",
    "\n",
    "#     raise NotImplementedError(\"\")\n",
    "\n",
    "# setattr(TalkingElevator, \"_respond\", _respond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _respond(self, intent_distrib: Dict[str, float], slots: Dict[str, str]):\n",
    "        \"\"\"Decides how to react based on user intents and detected slots.\"\"\"\n",
    "        intent = max(intent_distrib, key=intent_distrib.get, default=None)\n",
    "        if intent == 'move_to_floor':\n",
    "            if 'floor_number' in slots:\n",
    "                target_floor = slots['floor_number']\n",
    "                if target_floor == self.current_floor:\n",
    "                    self._say_to_user(\"You are already on this floor.\")\n",
    "                elif target_floor in self.next_floors:\n",
    "                    self.move_to_floor(target_floor)\n",
    "                    self.stop()\n",
    "                    self._say_to_user(f\"Arrived at floor {target_floor}.\")\n",
    "                    self.dialogue_history.append(f\"Moved to floor {target_floor}.\")\n",
    "                else:\n",
    "                    self._say_to_user(f\"I'm sorry, but floor {target_floor} is not on the list of next stops.\")\n",
    "            else:\n",
    "                self._say_to_user(\"Which floor would you like to go to?\")\n",
    "\n",
    "        elif intent == 'ask_next_floors':\n",
    "            if self.next_floors:\n",
    "                floors_str = ', '.join(str(floor) for floor in self.next_floors)\n",
    "                self._say_to_user(f\"The next stops are: {floors_str}.\")\n",
    "            else:\n",
    "                self._say_to_user(\"There are no more stops scheduled.\")\n",
    "\n",
    "        elif intent == 'clarification_request':\n",
    "            self._say_to_user(\"Could you please clarify your request?\")\n",
    "        \n",
    "        else:\n",
    "            self._say_to_user(\"I didn't quite understand that. Can you please repeat?\")\n",
    "\n",
    "        # Record dialogue history\n",
    "        self.dialogue_history.append(f\"User said: {slots}\")\n",
    "\n",
    "setattr(TalkingElevator, \"_respond\", _respond)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "The last step is to implement the `process_input` method in the `TalkingElevator` class. The method should rely on the intent recognition, slot filling and response selection mechanism (which you have implemented in the previous steps) to react to a given user input.\n",
    "\n",
    "**Task 1.6** (1 point): Implement the `process_input` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_input(self, user_input: str, conf_score:float=1.0):\n",
    "#     \"\"\"Processes the (transcribed) user input, and respond appropriately \n",
    "#     (through a verbal response and possibly also an action, such as moving floors).\n",
    "#     The method should rely on the intent classifier, slot-filling function, and\n",
    "#     response selection function.\"\"\"\n",
    "\n",
    "#     self._add_to_dialogue_history(user_input, speaker=\"user\", conf_score=conf_score)\n",
    "#     raise NotImplementedError()\n",
    "\n",
    "# setattr(TalkingElevator, \"process_input\", process_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(self, user_input: str, conf_score: float = 1.0):\n",
    "   \"\"\"Processes the (transcribed) user input and responds appropriately.\"\"\"\n",
    "   self._add_to_dialogue_history(user_input, speaker=\"user\", conf_score=conf_score)\n",
    "   intent_distrib = self.intent_recognition(user_input)\n",
    "   slots = self.fill_slots(user_input)\n",
    "   self._respond(intent_distrib, slots)\n",
    "   \n",
    "setattr(TalkingElevator, \"process_input\", process_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to test our talking elevator: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elevator = TalkingElevator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your talking elevator will mostly likely not function properly right from the start. Identify what works and what doesn't and correct the code you have developed in Tasks 1.1 - 1.6 until your system meets the specifications we have outlined. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 : Machine translation\n",
    "\n",
    "In this part, we evaluate a pre-trained machine translation model on data from the Lord of the Rings movies and fine-tune it to improve the translation quality.\n",
    "\n",
    "### Data\n",
    "\n",
    "We provide you with two files, `lotr.detok.de` and `lotr.detok.en`, containing German and English movie subtitles. These two files constitute a so-called _parallel corpus_, i.e. each sentence/line in German corresponds to a sentence/line in English. The two files have the same number of lines and the German sentence on line $i$ corresponds to the English sentence on line $i$. The subtitles are extracted from the [OpenSubtitles-2018](https://opus.nlpl.eu/OpenSubtitles/corpus/version/OpenSubtitles) corpus.\n",
    "\n",
    "Here are the first ten lines of the two files:\n",
    "\n",
    "<style scoped>\n",
    "table {\n",
    "  font-size: 12px;\n",
    "}\n",
    "</style>\n",
    "| Nb  | German (`lotr.detok.de`)         | English (`lotr.detok.en`)      |\n",
    "|---|----------------------------------|--------------------------------|\n",
    "| 1 | Die Welt ist im Wandel. | The world is changed.   |\n",
    "| 2 | Ich spüre es im Wasser. | I feel it in the water. |\n",
    "| 3 | Ich spüre es in der Erde. | I feel it in the earth. |\n",
    "| 4 | Ich rieche es in der Luft. | I smell it in the air. |\n",
    "| 5 | Vieles, was einst war, ist verloren, da niemand mehr lebt, der sich erinnert. | Much that once was is lost. For none now live who remember it. |\n",
    "| 6 | Es begann mit dem Schmieden der Großen Ringe. | It began with the forging of the Great Rings. |\n",
    "| 7 | 3 wurden den Elben gegeben, den unsterblichen, weisesten und reinsten aller Wesen. | Three were given to the Elves: Immortal, wisest and fairest of all beings. |\n",
    "| 8 | 7 den Zwergenherrschern, großen Bergleuten und Handwerkern in ihren Hallen aus Stein. | Seven to the Dwarf-lords: Great miners and craftsmen of the mountain halls. |\n",
    "| 9 | Und 9... 9 Ringe wurden den Menschen geschenkt, die vor allem anderen nach Macht streben. | And nine nine rings were gifted to the race of Men who, above all else, desire power. |\n",
    "| 10 | Denn diese Ringe bargen die Kraft und den Willen, jedes Volk zu leiten. | For within these rings was bound the strength and will to govern each race. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started\n",
    "\n",
    "We will a pretrained machine translation model for German-to-English translation. The model is available on the HuggingFace model hub and can be used with the `transformers` library.\n",
    "\n",
    "Let us first make sure that all required modules are installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch transformers accelerate evaluate sacrebleu sacremoses sentencepiece unbabel-comet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bilingual model is called [`opus-mt-de-en`](https://huggingface.co/Helsinki-NLP/opus-mt-de-en) and has been trained by the Helsinki-NLP group. Like (almost) all HuggingFace models, it consists of a _tokenizer_ and the _sequence-to-sequence model_ properly speaking. We need to load both separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fp/projects01/ec30/software/easybuild/software/nlpl-pytorch/2.1.2-foss-2022b-cuda-12.0.0-Python-3.10.8/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"helsinki-nlp/opus-mt-de-en\")\n",
    "translator = transformers.AutoModelForSeq2SeqLM.from_pretrained(\"helsinki-nlp/opus-mt-de-en\")\n",
    "\n",
    "# Change \"cuda\" to \"cpu\" if you're running on a machine without GPU\n",
    "device = \"cpu\"\n",
    "translator = translator.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `transformers` library will automatically download the models from the HuggingFace hub the first time you run this cell, so it may take a bit longer.\n",
    "\n",
    "Let's take the first two German sentences, tokenize them, and translate them to English:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   55,   401,    29,    49,  9012,     3,     0, 58100, 58100],\n",
      "        [  105,  2768,  1691,    18,    65,    49,   672,     3,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer([\"Die Welt ist im Wandel.\", \"Ich spüre es im Wasser.\"], return_tensors=\"pt\", padding=True)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[58100,    36,   360,    19,  7315,     3,     0, 58100, 58100, 58100],\n",
      "        [58100,    38,    85,  1595,    56,     5,     4,   616,     3,     0]])\n"
     ]
    }
   ],
   "source": [
    "outputs = translator.generate(**tokens.to(device), max_new_tokens=50)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2.1__ (1 point):\n",
    "- What do the numbers in the `input_ids` represent?\n",
    "- What is the effect of `padding=True`? How would the data look like if padding was disabled?\n",
    "- What does `max_new_tokens` do? Why do you think it is important to set this parameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The numbers in the input_ids tensor represent the tokenized input sentences. Each unique word or subword from the sentences is mapped to a specific integer ID according to the tokenizer's vocabulary. For example in the first output, the first row corresponds to the tokenized sentence \"Die Welt ist im Wandel.\"\n",
    "- With `padding=True` we ensures that all sequences in the batch are of the same length by adding padding tokens (usually represented by 0 or another designated ID) to shorter sequences. This is crucial for batch processing, as many models require fixed-size inputs. Here, the input_ids would not include the padding tokens, and the sequences would have varying lengths.\n",
    "- The `max_new_tokens` parameter specifies the maximum number of new tokens the model can generate during the translation process. It's important basically for controlling the length of the output and ensuring it does not exceed a certain limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get actual words by running the output through the `batch_decode` function of the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The world is changing.', 'I can feel it in the water.']\n"
     ]
    }
   ],
   "source": [
    "translations = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(translations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ We assume that you will run the translations from German to English. If you would like to work on the opposite translation direction (and feel comfortable evaluating the German output), you are welcome to do so. The corresponding bilingual model is called `opus-mt-en-de`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "921fa1c6fe474c3588131db1473bc69e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792f3dc0855e4cf68315c97c8a49c123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56fd2efcf42344faa49da9feafc5298a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/768k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53ead9def16c4c57b7bf113c8c4ce602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/797k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3a29e0506346f4bf33ed11577d958f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43c578977dd45e7b7ee0f0fd0bb44f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/298M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99e98b9eb264a5dada8e72ef10ebfad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   36,   360,    19,  7315,     3,     0, 58100, 58100, 58100],\n",
      "        [   38,    85,  1595,    56,     5,     4,   616,     3,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"helsinki-nlp/opus-mt-en-de\")\n",
    "translator = transformers.AutoModelForSeq2SeqLM.from_pretrained(\"helsinki-nlp/opus-mt-en-de\")\n",
    "\n",
    "device = \"cpu\"\n",
    "translator = translator.to(device)\n",
    "\n",
    "tokens = tokenizer([\"The world is changing.\", \"I can feel it in the water.\"], return_tensors=\"pt\", padding=True)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[58100,    55,   401,  6190,    63,     3,     0, 58100, 58100, 58100],\n",
      "        [58100,   105,  2768,  1691,    18,    65,    49,   672,     3,     0]])\n"
     ]
    }
   ],
   "source": [
    "outputs = translator.generate(**tokens.to(device), max_new_tokens=50)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Die Welt verändert sich.', 'Ich spüre es im Wasser.']\n"
     ]
    }
   ],
   "source": [
    "translations = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(translations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting\n",
    "\n",
    "Before we move on, we need to split our data. We will evaluate different models and for that we'll need test data. We will also fine-tune a model, and for that we'll need training data. The entire Lord of the Rings dataset has 9640 lines.\n",
    "\n",
    "__Task 2.2__ (1 point): Split the dataset in such a way that the **last** 1000 lines are used for testing and the remaining lines (8640) for training. Save the data under the following filenames: `lotr.train.de, lotr.train.en, lotr.test.de, lotr.test.en`. You can use Python code or other tools to perform the splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 9640\n"
     ]
    }
   ],
   "source": [
    "with open('lotr.detok.de', 'r', encoding='utf-8') as f:\n",
    "    german_lines = f.readlines()\n",
    "\n",
    "with open('lotr.detok.en', 'r', encoding='utf-8') as f:\n",
    "    english_lines = f.readlines()\n",
    "\n",
    "print(f'Total lines: {len(german_lines)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_german = german_lines[:-1000]\n",
    "test_german = german_lines[-1000:]\n",
    "\n",
    "train_english = english_lines[:-1000]\n",
    "test_english = english_lines[-1000:]\n",
    "\n",
    "# Saving\n",
    "with open('lotr.train.de', 'w', encoding='utf-8') as f:\n",
    "    f.writelines(train_german)\n",
    "\n",
    "with open('lotr.train.en', 'w', encoding='utf-8') as f:\n",
    "    f.writelines(train_english)\n",
    "\n",
    "with open('lotr.test.de', 'w', encoding='utf-8') as f:\n",
    "    f.writelines(test_german)\n",
    "\n",
    "with open('lotr.test.en', 'w', encoding='utf-8') as f:\n",
    "    f.writelines(test_english)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2.3__ (1 point): What are potential risks and drawbacks of splitting the dataset in this way? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting a dataset it's not a greate idea because there are a lot of potential issues. One major concern is temporal bias, especially if the dataset is ordered chronologically; the last lines may not represent the same distribution as the earlier ones, leading to a model that performs poorly on the test data due to context or style differences. This method can cause data leakage if dependencies exist between the training and testing sets, inflating performance metrics and misguiding evaluations. This split also lacks randomness, which could lead to unbalanced representations of the data in both sets, increasing the risk of overfitting. Maybe we can use stratified sampling or random shuffling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to translate the test set with our model.\n",
    "\n",
    "__Task 2.4__ (2 points): Create a function that loads the entire `lotr.test.de` file, translates each line with the `opus-mt-de-en` model and writes its output to a new file, one sentence per line.\n",
    "\n",
    "The easiest way to do this is to just load the entire test file into a list, tokenize and translate it, but the test set may be too large to fit on GPU memory, or it might be inefficient and slow if you use a CPU. A better alternative is to split the data into batches of 50-100 sentences and send each batch separately to the translator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: 100%|██████████| 10/10 [19:25<00:00, 116.51s/it]\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def translate(input_file, translation_file, tokenizer, translator, batch_size=100):\n",
    "    \"\"\"Translate an input file line by line using the loaded tokenizer and translator,\n",
    "    and write the translations to output_file.\"\"\"\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    translated_lines = []\n",
    "    total_batches = len(lines) // batch_size + (1 if len(lines) % batch_size != 0 else 0)\n",
    "\n",
    "    # tqdm for progress bar\n",
    "    for i in tqdm(range(0, len(lines), batch_size), total=total_batches, desc=\"Translating\"):\n",
    "        batch = lines[i:i + batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {key: value.to(translator.device) for key, value in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():  # Disable gradient calculation\n",
    "            outputs = translator.generate(**inputs)\n",
    "\n",
    "        translated_batch = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        translated_lines.extend(translated_batch)\n",
    "\n",
    "    with open(translation_file, 'w', encoding='utf-8') as f:\n",
    "        for line in translated_lines:\n",
    "            f.write(line + '\\n')\n",
    "\n",
    "translate(\"lotr.test.de\", \"lotr.output_opus.en\", tokenizer, translator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, open the output file and check that the translations look ok. In particular, the file should contain the expected number of lines and output should be in the expected language (English or German, depending on the chosen direction).\n",
    "\n",
    "__Task 2.5__ (1 point): Open both the output file and the reference translations (`lotr.test.en` if translating from German to English) and compare the first 20 lines. How would you rate the translations of the OPUS system on a scale from 1 (incomprehensible and/or completely different meaning) to 5 (grammatically correct and meaning fully preserved)? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm giving a 3/5 because the OPUS translations for the first 20 lines of the German text generally convey the intended meaning, but they fall short of fully preserving the nuances and fluidity expected in high-quality translations. Many phrases are translated too literally, resulting in awkward or unnatural English that disrupts readability. While the grammar is mostly correct, certain sentences contain structural issues, leading to somewhat fragmented output that affects fluency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "We can now evaluate the quality of our translations. In a first step, we perform _reference-based surface-level evaluation_  using the popular BLEU score. We can do that with the `sacrebleu` module. Below is a slightly reformatted example taken from the [SacreBLEU documentation](https://github.com/mjpost/sacrebleu/tree/master?tab=readme-ov-file#using-sacrebleu-from-python):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 45.07 70.6/42.9/36.4/37.5 (BP = 1.000 ratio = 1.000 hyp_len = 17 ref_len = 17)\n"
     ]
    }
   ],
   "source": [
    "from sacrebleu.metrics import BLEU\n",
    "\n",
    "reference = ['The dog bit the man.', 'It was not unexpected.', 'The man bit him first.']\n",
    "hypothesis = ['The dog bit the man.', \"It wasn't surprising.\", 'The man had just bitten him.']\n",
    "\n",
    "bleu_scorer = BLEU()\n",
    "# BLEU can deal with multiple references per sentence, but here we only have one, so we just enclose it in another set of brackets:\n",
    "score = bleu_scorer.corpus_score(hypothesis, [reference])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2.6__ (1 point): Load both the system output and the reference of your test set and compute the corpus-level BLEU score. Also compute the corpus-level chrF score. Which of the scores is higher?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus-level BLEU score: 0.98\n",
      "Corpus-level chrF score: 14.60\n"
     ]
    }
   ],
   "source": [
    "from sacrebleu.metrics import BLEU, CHRF\n",
    "\n",
    "def evaluate_bleu_chrf(hypothesis_file, reference_file):\n",
    "    bleu = BLEU()\n",
    "    chrf = CHRF()\n",
    "    \n",
    "    with open(hypothesis_file, 'r', encoding='utf-8') as hyp_file:\n",
    "        hypotheses = [line.strip() for line in hyp_file.readlines()]\n",
    "    with open(reference_file, 'r', encoding='utf-8') as ref_file:\n",
    "        references = [line.strip() for line in ref_file.readlines()]\n",
    "    \n",
    "    # scores\n",
    "    bleu_score = bleu.corpus_score(hypotheses, [references]).score\n",
    "    chrf_score = chrf.corpus_score(hypotheses, [references]).score\n",
    "    print(f\"Corpus-level BLEU score: {bleu_score:.2f}\")\n",
    "    print(f\"Corpus-level chrF score: {chrf_score:.2f}\")\n",
    "\n",
    "# Example usage\n",
    "evaluate_bleu_chrf(\"lotr.output_opus.en\", \"lotr.test.en\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Corpus-level chrF score is higher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides string-based metrics, neural metrics have become increasingly popular lately, since they have been shown to correlate better with human judgements. The most popular neural metric is called COMET and it can be used with the HuggingFace `evaluate` package. The example below is from the [documentation](https://huggingface.co/spaces/evaluate-metric/comet/blob/main/README.md):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-28 16:20:50.382173: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-28 16:20:50.382228: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-28 16:20:50.383636: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-28 16:20:50.391131: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-28 16:20:55.925585: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac45411bf53346dfaac376616b03c295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.97k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cd478ab0db14ee096a8d3671f5b0914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6dc3b7f6327437db00ccd5f11cc8156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648a1c18adec4f08b932bb4a78534c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b83e45cd80e420b9634249ac02d0ed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LICENSE:   0%|          | 0.00/9.69k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e75064aa1943daa8cb6d04a81a53a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hparams.yaml:   0%|          | 0.00/567 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f3497416e784f3faa4834796dc54398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.ckpt:   0%|          | 0.00/2.32G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint .cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/371e9839ca4e213dde891b066cf3080f75ec7e72/checkpoints/model.ckpt`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f40661a2fc24b2e9042b8366a0ac1de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef09e19ac0642dc902e21619964bd85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d8697e193a4a69a277b7ac0fd91601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "378662c7ddd1498b94a9183a2f690ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoder model frozen.\n",
      "/fp/homes01/u01/ec-gabrield/.local/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "/fp/homes01/u01/ec-gabrield/.local/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /cluster/software/EL9/easybuild/software/jupyter-ser ...\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_score': 0.9051420092582703, 'scores': [0.8385583162307739, 0.9717257022857666]}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "comet_metric = evaluate.load('comet')\n",
    "src = [\"Dem Feuer konnte Einhalt geboten werden\", \"Schulen und Kindergärten wurden eröffnet.\"]\n",
    "hyp = [\"The fire could be stopped\", \"Schools and kindergartens were open\"]\n",
    "ref = [\"They were able to control the fire.\", \"Schools and kindergartens opened\"]\n",
    "comet_score = comet_metric.compute(predictions=hyp, references=ref, sources=src)\n",
    "print(comet_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2.7__ (1 point): Adapt this code to evaluate the output of the OPUS model. Note that COMET also requires the source text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fp/homes01/u01/ec-gabrield/.local/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /cluster/software/EL9/easybuild/software/jupyter-ser ...\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMET Score: {'mean_score': 0.7155076871365309, 'scores': [0.9114322662353516, 0.5371808409690857, 0.7610780596733093, 0.702925443649292, 0.9578304886817932, 0.8175438046455383, 0.48584502935409546, 0.8657263517379761, 0.6912862062454224, 0.4729769229888916, 0.6021386384963989, 0.5424555540084839, 0.39106670022010803, 0.6396649479866028, 0.7489166259765625, 0.8702800273895264, 0.8097954988479614, 0.9071325063705444, 0.6414852142333984, 0.7230848073959351, 0.4791286587715149, 0.6162791848182678, 0.7267099022865295, 0.7105870246887207, 0.840977668762207, 0.7223947048187256, 0.7569876909255981, 0.3213455080986023, 0.558647096157074, 0.7251235246658325, 0.5417189598083496, 0.5329115986824036, 0.8773753643035889, 0.435845285654068, 0.2771272659301758, 0.30075937509536743, 0.5079101920127869, 0.5500728487968445, 0.44638627767562866, 0.4835907220840454, 0.8644342422485352, 0.7896643280982971, 0.577034592628479, 0.8151124715805054, 0.9198242425918579, 0.6831170916557312, 0.47418591380119324, 0.7224751710891724, 0.9863395094871521, 0.6630555391311646, 0.35132846236228943, 0.6778610348701477, 0.8450379967689514, 0.9515135884284973, 0.4481070041656494, 0.9418677091598511, 0.7473583817481995, 0.6762445569038391, 0.8911690711975098, 0.949923574924469, 0.761603593826294, 0.8881892561912537, 0.38394877314567566, 0.38394850492477417, 0.8354960680007935, 0.914452314376831, 0.9267487525939941, 0.5037310123443604, 0.8047158718109131, 0.9427231550216675, 0.6509770750999451, 0.8047159910202026, 0.8393431901931763, 0.8516703844070435, 0.8952305912971497, 0.8319424390792847, 0.800310492515564, 0.7035135626792908, 0.856633186340332, 0.6960962414741516, 0.8028990626335144, 0.7067697048187256, 0.8185424208641052, 0.6367454528808594, 0.5974968075752258, 0.7519834041595459, 0.7453986406326294, 0.894212007522583, 0.7029129266738892, 0.6580898761749268, 0.4716423749923706, 0.8361225724220276, 0.882627546787262, 0.837501049041748, 0.9683809280395508, 0.9131059646606445, 0.9035553336143494, 0.8698634505271912, 0.7107378244400024, 0.5796590447425842, 0.8444958329200745, 0.8166220188140869, 0.9365454912185669, 0.5258276462554932, 0.43708446621894836, 0.7542896270751953, 0.945695698261261, 0.5378968119621277, 0.6824676990509033, 0.8624034523963928, 0.9135701656341553, 0.8007861375808716, 0.5219737887382507, 0.5307998657226562, 0.7814494967460632, 0.8693946599960327, 0.7360509037971497, 0.6336888670921326, 0.858467698097229, 0.7315869331359863, 0.5048730373382568, 0.8285877704620361, 0.4381686747074127, 0.6399381160736084, 0.649723470211029, 0.8856319785118103, 0.4887057840824127, 0.7388826012611389, 0.949794352054596, 0.5110613107681274, 0.7401854395866394, 0.7878912091255188, 0.7290219068527222, 0.6932129859924316, 0.8530654907226562, 0.44001340866088867, 0.4972703456878662, 0.5635614395141602, 0.7872948050498962, 0.6567223072052002, 0.9392026662826538, 0.9830448627471924, 0.6285963654518127, 0.5772871971130371, 0.4949975311756134, 0.595657467842102, 0.8964565992355347, 0.8396717309951782, 0.7100358009338379, 0.7532945871353149, 0.7532951831817627, 0.7776747941970825, 0.7718878984451294, 0.7455929517745972, 0.6771661043167114, 0.9138176441192627, 0.4426996111869812, 0.8303256034851074, 0.86037677526474, 0.7653820514678955, 0.5409448146820068, 0.5663528442382812, 0.336814284324646, 0.757931649684906, 0.7167648077011108, 0.6785058379173279, 0.7240476012229919, 0.7045212984085083, 0.8042012453079224, 0.5977606773376465, 0.46794232726097107, 0.871837317943573, 0.6944513320922852, 0.6010255813598633, 0.7674053311347961, 0.7375838756561279, 0.8522486686706543, 0.8565241098403931, 0.7753990888595581, 0.6617879271507263, 0.9057406783103943, 0.5359935760498047, 0.803538978099823, 0.8339178562164307, 0.8977242708206177, 0.3706100881099701, 0.9021533727645874, 0.8761982917785645, 0.9564434289932251, 0.6698228120803833, 0.5235083699226379, 0.9439684152603149, 0.5892804265022278, 0.8216009140014648, 0.7511269450187683, 0.512614369392395, 0.5954932570457458, 0.519922137260437, 0.546485424041748, 0.8962733745574951, 0.6653377413749695, 0.5785121321678162, 0.8563665151596069, 0.4725413918495178, 0.8807587027549744, 0.613960862159729, 0.7262852787971497, 0.8081936836242676, 0.5216124057769775, 0.7354972958564758, 0.8178170919418335, 0.7519432902336121, 0.871189296245575, 0.8954136967658997, 0.43741968274116516, 0.6735081076622009, 0.6927404403686523, 0.7468571662902832, 0.914333701133728, 0.4266757667064667, 0.906575083732605, 0.7616037130355835, 0.7446362376213074, 0.8396717309951782, 0.5915251970291138, 0.6910669207572937, 0.5383045077323914, 0.6546601057052612, 0.4871683418750763, 0.4420969486236572, 0.8430105447769165, 0.8397876620292664, 0.48062101006507874, 0.6241860389709473, 0.600382924079895, 0.49099093675613403, 0.8327462077140808, 0.7354859113693237, 0.5970600843429565, 0.5259320735931396, 0.4955575466156006, 0.38588419556617737, 0.4979206621646881, 0.8072903752326965, 0.5289039611816406, 0.6409556865692139, 0.5792956948280334, 0.7333250045776367, 0.7032469511032104, 0.7381582856178284, 0.46193939447402954, 0.840644896030426, 0.46913179755210876, 0.5453153252601624, 0.8174721002578735, 0.8480549454689026, 0.6211625933647156, 0.45038551092147827, 0.764153242111206, 0.7777608633041382, 0.9318921566009521, 0.6667995452880859, 0.9094643592834473, 0.6925703287124634, 0.7440413236618042, 0.6464135050773621, 0.6868486404418945, 0.8034534454345703, 0.9362748861312866, 0.9830645322799683, 0.9107755422592163, 0.49225807189941406, 0.830813467502594, 0.6615076065063477, 0.619010329246521, 0.7251233458518982, 0.6731200218200684, 0.7418606281280518, 0.7089897394180298, 0.7146828174591064, 0.4764874577522278, 0.7043280005455017, 0.8684821128845215, 0.8218860626220703, 0.9277011156082153, 0.8704696297645569, 0.521679699420929, 0.890722393989563, 0.6580241918563843, 0.8199734687805176, 0.7805691957473755, 0.542964518070221, 0.5056504607200623, 0.7778612375259399, 0.9501876831054688, 0.69926518201828, 0.5844458341598511, 0.7029362916946411, 0.8544586896896362, 0.9144991040229797, 0.8046793937683105, 0.9168639183044434, 0.5396634340286255, 0.9154796600341797, 0.49515196681022644, 0.44125252962112427, 0.3440057635307312, 0.531761109828949, 0.8596079349517822, 0.4340876638889313, 0.8907639980316162, 0.5978835225105286, 0.6581208109855652, 0.4448487162590027, 0.5064192414283752, 0.8427293300628662, 0.7871809005737305, 0.43062227964401245, 0.8079811930656433, 0.7762721180915833, 0.7528786659240723, 0.8451299667358398, 0.8920990228652954, 0.8578752279281616, 0.9181482791900635, 0.4147607088088989, 0.9221651554107666, 0.7377170920372009, 0.9306666851043701, 0.9706108570098877, 0.5087051391601562, 0.33452075719833374, 0.6554553508758545, 0.8222742080688477, 0.6752184629440308, 0.7055988311767578, 0.48165205121040344, 0.41174641251564026, 0.7924433350563049, 0.669026255607605, 0.32573890686035156, 0.9276021122932434, 0.503932774066925, 0.9239681363105774, 0.41085144877433777, 0.6557936072349548, 0.5542787313461304, 0.8240529298782349, 0.7813777327537537, 0.4410916864871979, 0.9062435626983643, 0.7969642877578735, 0.7406470775604248, 0.5782639980316162, 0.6274701356887817, 0.5380803942680359, 0.5076523423194885, 0.5746723413467407, 0.6659800410270691, 0.7959253191947937, 0.7200779318809509, 0.6034371852874756, 0.8214172124862671, 0.508183479309082, 0.2785354256629944, 0.6962225437164307, 0.88017737865448, 0.8421403765678406, 0.7606023550033569, 0.41852790117263794, 0.4904613792896271, 0.8383089900016785, 0.8805269002914429, 0.5584540367126465, 0.5578490495681763, 0.7617270946502686, 0.5276588201522827, 0.4107293486595154, 0.6625875234603882, 0.5853862762451172, 0.8873571753501892, 0.8593497276306152, 0.6107038855552673, 0.9012770652770996, 0.8884679675102234, 0.8876421451568604, 0.3948248326778412, 0.6347485780715942, 0.8459870219230652, 0.8273553848266602, 0.4926226735115051, 0.761603593826294, 0.6902560591697693, 0.7790708541870117, 0.7328192591667175, 0.837756335735321, 0.3965682089328766, 0.5680180191993713, 0.6033735871315002, 0.8802443742752075, 0.8265249729156494, 0.83379065990448, 0.6658622026443481, 0.6814619302749634, 0.717565655708313, 0.7208641171455383, 0.6265305876731873, 0.781256914138794, 0.8267831206321716, 0.7751181125640869, 0.8743135929107666, 0.8728941679000854, 0.8581488728523254, 0.5814979076385498, 0.7613988518714905, 0.7316139936447144, 0.82135009765625, 0.35169893503189087, 0.8703395128250122, 0.6818789839744568, 0.7153728008270264, 0.31149569153785706, 0.6484574675559998, 0.4744572043418884, 0.3097643256187439, 0.7623755931854248, 0.583105206489563, 0.9136762619018555, 0.5655093789100647, 0.544991135597229, 0.5926650762557983, 0.6142727136611938, 0.9275960922241211, 0.709621012210846, 0.447462797164917, 0.5526542067527771, 0.45096778869628906, 0.6167621612548828, 0.8806006908416748, 0.8822375535964966, 0.9031466841697693, 0.7391557693481445, 0.7235609292984009, 0.7005246877670288, 0.8655675649642944, 0.6230170726776123, 0.8972266912460327, 0.7484660148620605, 0.6301364898681641, 0.7696456909179688, 0.799465537071228, 0.6750450134277344, 0.7160112261772156, 0.9112734794616699, 0.5124372243881226, 0.3161509037017822, 0.7549313902854919, 0.7953658103942871, 0.8135320544242859, 0.81854248046875, 0.7594107389450073, 0.368933767080307, 0.5490680932998657, 0.7912116050720215, 0.7206436395645142, 0.7972323894500732, 0.658885657787323, 0.6166292428970337, 0.9295118451118469, 0.6984996199607849, 0.7207950949668884, 0.5289974212646484, 0.43856608867645264, 0.8701795935630798, 0.6828915476799011, 0.9706108570098877, 0.5070117712020874, 0.7279673218727112, 0.5319200158119202, 0.685981810092926, 0.6024268269538879, 0.7032836675643921, 0.6110574007034302, 0.603190004825592, 0.7843255400657654, 0.4433743357658386, 0.9000184535980225, 0.9029804468154907, 0.8821565508842468, 0.7911542654037476, 0.9393354654312134, 0.8534994125366211, 0.6981946229934692, 0.39170804619789124, 0.5735626816749573, 0.5412148237228394, 0.7729396820068359, 0.9135837554931641, 0.55321204662323, 0.6932538747787476, 0.47083064913749695, 0.8657262325286865, 0.8831620216369629, 0.700788140296936, 0.5625841021537781, 0.7518292665481567, 0.5354130268096924, 0.8258103132247925, 0.7816650867462158, 0.643835186958313, 0.6801328659057617, 0.5120521187782288, 0.8343442678451538, 0.5958592891693115, 0.7230186462402344, 0.8511862754821777, 0.7743088603019714, 0.7230716943740845, 0.8967698812484741, 0.4630129933357239, 0.6523604393005371, 0.6542178988456726, 0.6796560287475586, 0.8720239400863647, 0.729678750038147, 0.662369430065155, 0.8795071840286255, 0.6701114177703857, 0.8033671975135803, 0.6449601650238037, 0.7360565066337585, 0.8625417947769165, 0.7943280339241028, 0.6162632703781128, 0.9363865852355957, 0.6309007406234741, 0.924493670463562, 0.7391600608825684, 0.950432300567627, 0.9295661449432373, 0.8053795099258423, 0.9802058935165405, 0.686564564704895, 0.6484459042549133, 0.7284778356552124, 0.9795013070106506, 0.7374463081359863, 0.3348153531551361, 0.8102721571922302, 0.8756729364395142, 0.3661472499370575, 0.9639922380447388, 0.9550579786300659, 0.9452310800552368, 0.7361999750137329, 0.8322668075561523, 0.9441711902618408, 0.3661472201347351, 0.8756728172302246, 0.39875006675720215, 0.7703022956848145, 0.44060423970222473, 0.6162397265434265, 0.7510689496994019, 0.8102721571922302, 0.8262079954147339, 0.8613312244415283, 0.7740909457206726, 0.7391597628593445, 0.9521714448928833, 0.8240277767181396, 0.8050190210342407, 0.8050193190574646, 0.8682219982147217, 0.7498796582221985, 0.40483415126800537, 0.8756730556488037, 0.6175733208656311, 0.9029554128646851, 0.7156420350074768, 0.4562779366970062, 0.903410792350769, 0.5051046013832092, 0.9104998707771301, 0.6852133274078369, 0.6504685282707214, 0.8627844452857971, 0.6354986429214478, 0.4251401424407959, 0.9097167253494263, 0.8224790096282959, 0.7383221387863159, 0.7500625848770142, 0.610174298286438, 0.8708473443984985, 0.7720332145690918, 0.5985532999038696, 0.6574649810791016, 0.7576302886009216, 0.6047642827033997, 0.7460139393806458, 0.9474301338195801, 0.9456585645675659, 0.50228351354599, 0.7946020364761353, 0.7678837776184082, 0.6552007794380188, 0.5229991674423218, 0.7594273090362549, 0.9101404547691345, 0.5255110859870911, 0.7875691652297974, 0.7957790493965149, 0.4875580370426178, 0.8086176514625549, 0.8338191509246826, 0.7561931610107422, 0.7132525444030762, 0.5998572707176208, 0.6585768461227417, 0.5686378479003906, 0.415702760219574, 0.59173983335495, 0.9495092630386353, 0.9365307092666626, 0.7930908203125, 0.8237469792366028, 0.31966260075569153, 0.5327013731002808, 0.8774914741516113, 0.5863449573516846, 0.7261782884597778, 0.7418075203895569, 0.4409167170524597, 0.4088762402534485, 0.9135701656341553, 0.9154796600341797, 0.6278401017189026, 0.4648859202861786, 0.6752496957778931, 0.6733564734458923, 0.8616392612457275, 0.6170253753662109, 0.9034456610679626, 0.8381304144859314, 0.8381304144859314, 0.8388139605522156, 0.5614262819290161, 0.9252337217330933, 0.48755767941474915, 0.7011789679527283, 0.45316755771636963, 0.7011789679527283, 0.8508247137069702, 0.9295118451118469, 0.8381304144859314, 0.7250240445137024, 0.8381304144859314, 0.6876147985458374, 0.8407817482948303, 0.7730693817138672, 0.8407818078994751, 0.4675741195678711, 0.7574968934059143, 0.9072262048721313, 0.81854248046875, 0.7015079259872437, 0.8528165221214294, 0.634566605091095, 0.7791290283203125, 0.7841856479644775, 0.9830448627471924, 0.8302949666976929, 0.8302949666976929, 0.7629878520965576, 0.6707035899162292, 0.7064047455787659, 0.6693934202194214, 0.6949363350868225, 0.7351698875427246, 0.8175767660140991, 0.7183110117912292, 0.9156707525253296, 0.5223470330238342, 0.9025751352310181, 0.8222784996032715, 0.8933483362197876, 0.7714986801147461, 0.8759415149688721, 0.7792998552322388, 0.8166428804397583, 0.5057358145713806, 0.992354154586792, 0.9051007032394409, 0.6989723443984985, 0.9020611047744751, 0.8629937171936035, 0.581575870513916, 0.6783550381660461, 0.9086668491363525, 0.7251233458518982, 0.6248424649238586, 0.33438625931739807, 0.7832662463188171, 0.8306747674942017, 0.6402717232704163, 0.3839486837387085, 0.9167547225952148, 0.9066265821456909, 0.8401930928230286, 0.44616106152534485, 0.3628726899623871, 0.7539427280426025, 0.3839484453201294, 0.9863395094871521, 0.7956335544586182, 0.7431154251098633, 0.7653644680976868, 0.7286407351493835, 0.8596073389053345, 0.9079894423484802, 0.5900079011917114, 0.7399047017097473, 0.8265799283981323, 0.9315832853317261, 0.5542790293693542, 0.8148058652877808, 0.893245279788971, 0.9284493923187256, 0.7814314961433411, 0.50228351354599, 0.8584859371185303, 0.8954579830169678, 0.8664895296096802, 0.8199926614761353, 0.5577359199523926, 0.5943372845649719, 0.7772524952888489, 0.5502550005912781, 0.8478294610977173, 0.6962892413139343, 0.6896133422851562, 0.6486985087394714, 0.8252480030059814, 0.9539365768432617, 0.9539365768432617, 0.7496330142021179, 0.7812731862068176, 0.7216184139251709, 0.7035849690437317, 0.9071270227432251, 0.8721975684165955, 0.5080649852752686, 0.8881598711013794, 0.8290002346038818, 0.7636610865592957, 0.6571865677833557, 0.9767876863479614, 0.5260956287384033, 0.6799854040145874, 0.9047063589096069, 0.8395583033561707, 0.9229790568351746, 0.6566157937049866, 0.35169777274131775, 0.795161247253418, 0.7813775539398193, 0.7334538698196411, 0.7688611745834351, 0.945950984954834, 0.6899697780609131, 0.5067365765571594, 0.7759968638420105, 0.6563421487808228, 0.881713330745697, 0.40910446643829346, 0.9082496166229248, 0.8199267387390137, 0.4721141457557678, 0.9029169082641602, 0.2438054233789444, 0.6599553823471069, 0.7628569602966309, 0.7785520553588867, 0.606951892375946, 0.8558891415596008, 0.6840106844902039, 0.49184972047805786, 0.9552695751190186, 0.5391846895217896, 0.7948994636535645, 0.8805737495422363, 0.40505459904670715, 0.714990496635437, 0.6205871105194092, 0.9830448627471924, 0.7891387343406677, 0.798290491104126, 0.6231830716133118, 0.5859376192092896, 0.8925023674964905, 0.540622889995575, 0.8290187120437622, 0.47369882464408875, 0.48681408166885376, 0.81854248046875, 0.6497713923454285, 0.7918275594711304, 0.900143027305603, 0.3657078146934509, 0.8257234692573547, 0.8586140871047974, 0.9216635823249817, 0.3173196315765381, 0.737564206123352, 0.8374453783035278, 0.23778420686721802, 0.6159949898719788, 0.8893545269966125, 0.35824763774871826, 0.9830448627471924, 0.9830448627471924, 0.81854248046875, 0.8185424208641052, 0.3089917004108429, 0.3582475483417511, 0.35824763774871826, 0.9938724637031555, 0.3089917004108429, 0.9706108570098877, 0.9706108570098877, 0.36266347765922546, 0.9938724637031555, 0.7233927845954895, 0.7891386151313782, 0.7186651825904846, 0.7186647057533264, 0.8361572623252869, 0.7711914777755737, 0.3626634478569031, 0.36266350746154785, 0.9012966156005859, 0.818605363368988, 0.3811498284339905, 0.4097500741481781, 0.8185423612594604, 0.5844234228134155, 0.7329709529876709, 0.6540041565895081, 0.8240314722061157, 0.7006867527961731, 0.9206792712211609, 0.7652456760406494, 0.5573717951774597, 0.5424840450286865, 0.6770929098129272, 0.5472341179847717, 0.754580020904541, 0.8897545337677002, 0.7259155511856079, 0.8672329187393188, 0.8723537921905518, 0.5711503028869629, 0.8185423612594604, 0.91567063331604, 0.8185423612594604, 0.893393874168396, 0.7129022479057312, 0.9830448627471924, 0.9830448627471924, 0.8207888603210449, 0.7074903845787048, 0.5073582530021667, 0.5073580741882324, 0.6916306018829346, 0.990216076374054, 0.7031304240226746, 0.9110057950019836, 0.9397612810134888, 0.9185799360275269, 0.8687655329704285, 0.7158691883087158, 0.7256768345832825, 0.7185424566268921, 0.9119563698768616, 0.7564635276794434, 0.9493012428283691, 0.5505553483963013, 0.5429873466491699, 0.9863395094871521, 0.8662250638008118, 0.7930956482887268, 0.8783036470413208, 0.7919066548347473, 0.761603593826294, 0.8456838130950928, 0.37465840578079224, 0.9067426323890686, 0.8560374975204468, 0.7406030297279358, 0.7801268696784973, 0.690878689289093, 0.7915586233139038, 0.503474235534668, 0.8756391406059265, 0.7420284748077393, 0.8271743655204773, 0.5227628946304321, 0.6161277294158936, 0.797533392906189, 0.5617467164993286, 0.6495846509933472, 0.8849818706512451, 0.3577565848827362, 0.769811749458313, 0.648320198059082, 0.8419848680496216, 0.5518494248390198, 0.6939340829849243, 0.8212363123893738, 0.5400235652923584, 0.7092611789703369, 0.5116196274757385, 0.5870221853256226, 0.8149834275245667, 0.6997097730636597, 0.668036162853241, 0.5971761345863342, 0.6438068747520447, 0.7103227972984314, 0.8739312887191772, 0.8480654954910278, 0.782740592956543, 0.7447384595870972, 0.4276941120624542, 0.7880212664604187, 0.38587310910224915, 0.705318808555603, 0.433668851852417, 0.7685106992721558, 0.9188019037246704, 0.8637347221374512, 0.8893144130706787, 0.4052238464355469, 0.9324301481246948, 0.5397225618362427, 0.8648703694343567, 0.9396293759346008, 0.43638473749160767, 0.5938262939453125, 0.3987906873226166, 0.5612491369247437, 0.7062982320785522, 0.6088655591011047, 0.5895265936851501, 0.3504161536693573, 0.7258286476135254, 0.7258286476135254, 0.8686118125915527, 0.9539687633514404, 0.49744129180908203, 0.8662647604942322, 0.9164535999298096, 0.941914439201355, 0.7653418779373169, 0.7646347284317017, 0.7611812353134155, 0.5927678942680359, 0.7851842641830444, 0.67317134141922, 0.6227040886878967, 0.8375009894371033, 0.6946790814399719, 0.8320227861404419, 0.7689895629882812, 0.5456121563911438, 0.5705164670944214, 0.6631113290786743, 0.8432725667953491, 0.8122010827064514, 0.660722553730011, 0.7616037726402283, 0.9196006059646606, 0.6789745688438416, 0.8893716931343079, 0.8750289082527161, 0.937404990196228, 0.9739823341369629, 0.7176517248153687, 0.8441307544708252, 0.9919137358665466, 0.8273553848266602, 0.9040566682815552, 0.43798214197158813, 0.8838119506835938, 0.8621634244918823, 0.8698431253433228, 0.8465424180030823, 0.9199547171592712, 0.9199547171592712]}\n"
     ]
    }
   ],
   "source": [
    "def evaluate_comet(hypothesis_file, reference_file, source_file):\n",
    "\n",
    "    with open(hypothesis_file, 'r', encoding='utf-8') as hyp_file:\n",
    "        hypotheses = [line.strip() for line in hyp_file.readlines()]\n",
    "    \n",
    "    with open(reference_file, 'r', encoding='utf-8') as ref_file:\n",
    "        references = [line.strip() for line in ref_file.readlines()]\n",
    "    \n",
    "    with open(source_file, 'r', encoding='utf-8') as src_file:\n",
    "        sources = [line.strip() for line in src_file.readlines()]\n",
    "    \n",
    "    comet_score = comet_metric.compute(predictions=hypotheses, references=references, sources=sources)\n",
    "    print(f\"COMET Score: {comet_score}\")\n",
    "\n",
    "# Example usage\n",
    "evaluate_comet(\"lotr.output_opus.en\", \"lotr.test.en\", \"lotr.test.de\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning\n",
    "\n",
    "Let us see now if we can further improve the translation quality. We still haven't used the training set after all...\n",
    "\n",
    "Fine-tuning a translation model with the `transformers` library is a bit convoluted. You need the following ingredients:\n",
    "- A `Seq2SeqTrainer` object, which defines the initial model and its tokenizer, the training data, and the configuration parameters (as a `Seq2SeqTrainingArguments` object). The training process starts with the `train()` method.\n",
    "- A `Seq2SeqTrainingArguments` object, which contains the configuration parameters, such as the number of training epochs, the path for saving the fine-tuned model, the learning rate etc.\n",
    "- A `DataCollatorForSeq2Seq` object that takes care of splitting the training data into batches of appropriate size.\n",
    "- A `DatasetDict` object containing the tokenized training data. Typically, the untokenized data is loaded into a `DatasetDict` object, and the tokenization function is applied to everything inside this `DatasetDict` using the `map()` function.\n",
    "\n",
    "__Task 2.8__ (1 point): The code in the box below shows a working example using the pretrained OPUS model, but is limited to two sentence pairs. Complete the code to load the entire training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "max_length = 100\n",
    "\n",
    "ds = Dataset.from_dict({\n",
    "    \"src_text\": [\"Die Welt ist im Wandel.\", \"Ich spüre es im Wasser.\"],\n",
    "    \"tgt_text\": [\"The world is changed.\", \"I feel it in the water.\"]\n",
    "})\n",
    "data = DatasetDict({\"train\": ds})\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(examples[\"src_text\"], text_target=examples[\"tgt_text\"], max_length=max_length, truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = data.map(preprocess_function, batched=True)\n",
    "print(tokenized_datasets)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=translator)\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"opus-mt-de-en-lotr\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    translator,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ea21642b714958939d7f3316b9bce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8640 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['src_text', 'tgt_text', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 8640\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='810' max='810' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [810/810 26:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.690500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[58100]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[58100]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[58100]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=810, training_loss=2.5166052547501927, metrics={'train_runtime': 1612.0746, 'train_samples_per_second': 16.079, 'train_steps_per_second': 0.502, 'total_flos': 248890269892608.0, 'train_loss': 2.5166052547501927, 'epoch': 3.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "max_length = 100\n",
    "\n",
    "with open(\"lotr.train.de\", \"r\", encoding=\"utf-8\") as src_file:\n",
    "    src_text = [line.strip() for line in src_file.readlines()]\n",
    "\n",
    "with open(\"lotr.train.en\", \"r\", encoding=\"utf-8\") as tgt_file:\n",
    "    tgt_text = [line.strip() for line in tgt_file.readlines()]\n",
    "\n",
    "ds = Dataset.from_dict({\"src_text\": src_text, \"tgt_text\": tgt_text})\n",
    "data = DatasetDict({\"train\": ds})\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"src_text\"], text_target=examples[\"tgt_text\"],\n",
    "        max_length=max_length, truncation=True\n",
    "    )\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenization\n",
    "tokenized_datasets = data.map(preprocess_function, batched=True)\n",
    "print(tokenized_datasets)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=translator)\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"opus-mt-de-en-lotr\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    translator,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model was fine-tuned for three epochs, and you should have three checkpoints in the `opus-mt-de-en-lotr` directory.\n",
    "\n",
    "__Task 2.9__ (1 point): Choose one of the checkpoints and use it to translate the test set. Evaluate the test set with BLEU, chrF and COMET. Note that locally saved model files (and tokenizers) can be loaded in the same way as models from the HuggingFace hub, e.g. with the following command: `transformers.AutoModelForSeq2SeqLM.from_pretrained(\"opus-mt-de-en-lotr/checkpoint-810\")`\n",
    "\n",
    "Did fine-tuning help? Did fine-tuning help? Have a look at the first rows of the files. Do you agree with the metrics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:33<00:00, 21.39s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289144b462b84202ae845774c1800ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint .cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/371e9839ca4e213dde891b066cf3080f75ec7e72/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "checkpoint_path = \"opus-mt-de-en-lotr/checkpoint-540\"  # checkpoint\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "translator = transformers.AutoModelForSeq2SeqLM.from_pretrained(checkpoint_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def translate_test_set(input_file, output_file, tokenizer, translator, batch_size=100):\n",
    "    with open(input_file, 'r', encoding='utf-8') as src_file:\n",
    "        src_lines = [line.strip() for line in src_file.readlines()]\n",
    "\n",
    "    translated_lines = []\n",
    "    for i in tqdm(range(0, len(src_lines), batch_size)):\n",
    "        batch = src_lines[i:i + batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(translator.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = translator.generate(**inputs)\n",
    "        translated_lines.extend(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as out_file:\n",
    "        for line in translated_lines:\n",
    "            out_file.write(line + \"\\n\")\n",
    "\n",
    "translate_test_set(\"lotr.test.de\", \"lotr.finetuned_output.en\", tokenizer, translator)\n",
    "\n",
    "bleu = evaluate.load('sacrebleu')\n",
    "chrf = evaluate.load('chrf')\n",
    "comet = evaluate.load('comet')\n",
    "\n",
    "with open(\"lotr.finetuned_output.en\", 'r', encoding='utf-8') as hyp_file:\n",
    "    hypotheses = [line.strip() for line in hyp_file.readlines()]\n",
    "\n",
    "with open(\"lotr.test.en\", 'r', encoding='utf-8') as ref_file:\n",
    "    references = [line.strip() for line in ref_file.readlines()]\n",
    "\n",
    "with open(\"lotr.test.de\", 'r', encoding='utf-8') as src_file:\n",
    "    sources = [line.strip() for line in src_file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 12.78\n",
      "chrF score: 32.35\n"
     ]
    }
   ],
   "source": [
    "# Compute and print BLEU, chrF, and COMET scores\n",
    "flat_references = [ref for sublist in references for ref in sublist]\n",
    "\n",
    "bleu_score = bleu.compute(predictions=hypotheses, references=[[ref] for ref in references])  # Wrap references\n",
    "print(f\"BLEU score: {bleu_score['score']:.2f}\")\n",
    "\n",
    "chrf_score = chrf.compute(predictions=hypotheses, references=[[ref] for ref in references])\n",
    "print(f\"chrF score: {chrf_score['score']:.2f}\")\n",
    "\n",
    "#comet_score = comet.compute(predictions=hypotheses, references=flat_references, sources=sources)\n",
    "#print(f\"COMET score: {comet_score['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The evaluation of the fine-tuned translation model yielded a BLEU score of 12.78 and a chrF score of 32.35, suggesting modest performance.\n",
    "#While some sentences convey a sense of meaning, many translations exhibit grammatical errors and awkward phrasing, indicating a lack of fluency and coherence.\n",
    "#For example, phrases like \"This stone could overall his\" and \"The Arkenstone lieves in this halls\" demonstrate clear mistranslations and could confuse readers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
