{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN4080: obligatory assignment 3\n",
    " \n",
    "Mandatory assignment 3 is about the practical use of Large Language Models (LLMs). More specifically, you will be tasked to implement a RAG (Retrieval-Augmented Generation) system able to answer factual questions based on a document database, more specifically Wiki pages extracted from an [online Star Wars encyclopedia](https://starwars.fandom.com). \n",
    "\n",
    "You are required to get at least 12/20 points to pass. \n",
    "\n",
    "- We assume that you have read and are familiar with IFI’s requirements and guidelines for mandatory assignments, see [here](https://www.uio.no/english/studies/examinations/compulsory-activities/mn-ifi-mandatory.html) and [here](https://www.uio.no/english/studies/examinations/compulsory-activities/mn-ifi-guidelines.html).\n",
    "- This is an individual assignment. You should not deliver joint submissions. \n",
    "- You may redeliver in Devilry before the deadline (__Sunday, October 13 at 23:59__).\n",
    "- Only the last delivery will be read! If you deliver more than one file, put them into a zip-archive. You don't have to include in your delivery the data files already provided for this assignment. \n",
    "- Name your submission _your\\_username\\_in4080\\_mandatory\\_3_\n",
    "\n",
    "The preferred way to complete this assignment is using the high-performance computing cluster _Fox_. See [here](https://www.uio.no/studier/emner/matnat/ifi/IN4080/h24/computing-setup.html) for instructions on how to register and log in to Fox.\n",
    "\n",
    "You should deliver a completed version of this Jupyter notebook, containing both your code and explanations about the steps you followed. We want to stress that simply submitting code is __not__ by itself sufficient to complete the assignment - we expect the notebook to also contain explanations of what you have implemented, along with motivations for the choices you made along the way. Preferably use whole sentences, and mathematical formulas if necessary. Explaining in your own words (using concepts we have covered through in the lectures) what you have implemented and reflecting on your solution is an important part of the learning process - take it seriously!\n",
    "\n",
    "Regarding the use of LLMs (ChatGPT or similar): you are allowed to use them as 'sparring partner', for instance to clarify something you have not understood. However, you are __not__ allowed to use them to generate solutions (either in part or in full) to the assignment tasks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic setup\n",
    "\n",
    "We will start by building a chatbot that directly answers user questions using an instruction-tuned LLM, without relying on any database. We will use the instruction-tuned version of the [Gemma 1.1 language model](https://huggingface.co/google/gemma-1.1-2b-it) from Google, which is available on HuggingFace. \n",
    "\n",
    "_Note: feel free to switch to another model (such as the newly released Llama 3 models) if you wish to experiment with them. Note, however, that the most recent LLMs will likely require a newer version of the `transformers` library than what is currently installed on Fox._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1** (4 points): Drawing inspiration from the code examples on the [Gemma webpage](https://huggingface.co/google/gemma-1.1-2b-it), implement the `__init__` and `get_response` methods. If you run the code on Fox with a GPU (or on a personal machine with a GPU), make sure that your code actually runs on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /fp/homes01/u01/ec-gabrield/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token='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')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "class BasicResponseGenerator:\n",
    "\n",
    "    def __init__(self, model_name=\"google/gemma-1.1-2b-it\"):\n",
    "        \"\"\"Loads the tokenizer and pretrained causal LM for the given model. \n",
    "        If a GPU is available, the model should be loaded on the GPU \"\"\"\n",
    "\n",
    "        raise NotImplemented(\"You must implement this method\")\n",
    "    \n",
    "    def get_response(self, prompt:str, max_length:int=50) -> str:\n",
    "        \"\"\"Given a prompt, generate a response (of a maximum max_length tokens) and return it.\n",
    "        Only the response should be returned, not the text of the prompt itself\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplemented(\"You must implement this method\")\n",
    "\n",
    "\n",
    "agent = BasicResponseGenerator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc2fa7ea0eb48e3bb2613b44f2f7724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "class BasicResponseGenerator:\n",
    "\n",
    "    def __init__(self, model_name=\"google/gemma-1.1-2b-it\", token=None):\n",
    "        \"\"\"Loads the tokenizer and pretrained causal LM for the given model.\n",
    "        If a GPU is available, the model should be loaded on the GPU.\"\"\"\n",
    "        \n",
    "        self.model_name = model_name\n",
    "\n",
    "        # Debugging error login\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(self.model_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "        # GPU if available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def get_response(self, prompt: str, max_length: int = 50) -> str:\n",
    "        \"\"\"Given a prompt, generate a response (of a maximum max_length tokens) and return it.\n",
    "        Only the response should be returned, not the text of the prompt itself.\"\"\"\n",
    "\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n",
    "\n",
    "        # Generate resp\n",
    "        output_ids = self.model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            pad_token_id=self.tokenizer.eos_token_id)\n",
    "\n",
    "        response = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        response = response[len(prompt):].strip()\n",
    "        return response\n",
    "        \n",
    "agent = BasicResponseGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: An easy way to verify that the GPU is actually used is to run the command `nvidia-smi` while your code is running. There also exists alternative GPU monitoring tools, like [`gpustat`](https://pypi.org/project/gpustat/0.3.2/)._\n",
    "\n",
    "You can then test your response generator with the following set of questions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who is Luke Skywalker?\n",
      "Answer: Luke Skywalker is a fictional character in the Star Wars franchise, a member of the Skywalker family, and a key figure in the Star Wars saga. He is the son of Anakin Skywalker and Padmé Amidala, and\n",
      "-------\n",
      "Question: Where is the Niima Outpost in Star Wars?\n",
      "Answer: The Niima Outpost is not mentioned in the Star Wars universe, so it does not exist.\n",
      "-------\n",
      "Question: Have you heard of Nute Gunray? Who is he?\n",
      "Answer: I am unable to find any information about Nute Gunray on the internet.\n",
      "-------\n",
      "Question: What kind of planet is Kashyyyk, and who discovered it?\n",
      "Answer: **Kashyyyk** is a fictional planet from the Star Wars universe. It is a forested planet located in the Outer Rim.\n",
      "\n",
      "**Kashyyyk was discovered by\n",
      "-------\n",
      "Question: Who are Condlurans, and can you give 2-3 names of known Condlurans?\n",
      "Answer: The term \"Condlurans\" is derived from the Greek word \"kondilos,\" which means \"to scrape.\"\n",
      "\n",
      "**Answer:**\n",
      "-------\n",
      "Question: What can you tell me about the First Battle of Geonosis?\n",
      "Answer: The First Battle of Geonosis was a pivotal battle in the Clone Wars, fought on the planet Geonosis. It was a decisive victory for the Republic, led\n",
      "-------\n",
      "Question: What is the name of the settlement where Anakin Skywalker and his mother lived?\n",
      "Answer: Anakin Skywalker and his mother lived on the planet **Babel-Babel**.\n",
      "-------\n",
      "Question: Which planet did Darth Sidious represent as senator?\n",
      "Answer: This question cannot be answered because Darth Sidious is a fictional character and does not represent any real-world entity.\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "questions = [\"Who is Luke Skywalker?\",\n",
    "             \"Where is the Niima Outpost in Star Wars?\",\n",
    "             \"Have you heard of Nute Gunray? Who is he?\",\n",
    "             \"What kind of planet is Kashyyyk, and who discovered it?\",\n",
    "             \"Who are Condlurans, and can you give 2-3 names of known Condlurans?\",\n",
    "             \"What can you tell me about the First Battle of Geonosis?\",\n",
    "             \"What is the name of the settlement where Anakin Skywalker and his mother lived?\",\n",
    "             \"Which planet did Darth Sidious represent as senator?\"]\n",
    "\n",
    "for question in questions:\n",
    "    print(\"Question:\", question)\n",
    "    print(\"Answer:\", agent.get_response(question))\n",
    "    print(\"-------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation is correct, the model should give you a few correct answers, but also many responses for which the model is either unable to give a precise answer, or hallucinates a (wrong) answer. This is expected, as the model is relatively small (3 billion parameters) and is a generic model that is not particularly optimised to generate trivia about the Star Wars Franchise. We will now try to improve the model performance by coupling the LLM to a document database.\n",
    "\n",
    "## Retrieval step\n",
    "\n",
    "Retrieval-augmented generation operates on a simple idea: instead of directly generating a response based on the \"parametric knowledge\" of the LLM, we first search for relevant documents in a database (or on the web). We then include the most relevant documents to the prompt, and ask the LLM to answer the user question _based on this retrieved knowledge_. \n",
    "\n",
    "In this assignment, you will use a set of Wiki texts extracted from an [online Star Wars encyclopedia](https://starwars.fandom.com) as document database. The wiki texts are available as a JSON file, either [here](https://home.nr.no/~plison/data/starwars.json) or on Fox at `/fp/projects01/ec403/IN4080/starwars.json`. The JSON is simply a dictionary mapping Wiki page titles to their content (in plain text).\n",
    "\n",
    "### Sparse retrieval \n",
    "\n",
    "We can start by using the newly released [BM25s](https://bm25s.github.io/) library, which implements a number of well-known search algorithms, which are all variants of the original [BM25 algorithm](https://en.wikipedia.org/wiki/Okapi_BM25) . Although BM25 is an old-fashioned search technique based on bag-of-words, it remains suprisingly effective, and is still widely used in modern NLP systems.\n",
    "\n",
    "**Task 2** (4 points): Fill in the implementation for the `BM25Retriever` class using [BM25s](https://bm25s.github.io/) (see the library documentation for details). You should filter out stop words by adding `stopwords='en_plus'` to the arguments of the tokenizer. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import bm25s\n",
    "import json\n",
    "from typing import List\n",
    "\n",
    "class BM25Retriever:\n",
    "\n",
    "    def __init__(self, filename=\"/fp/projects01/ec403/IN4080/starwars.json\"):\n",
    "        \"\"\"Using the json file provided as input, create a BM25s retriever \n",
    "        containing all (indexed) documents.\"\"\"\n",
    "\n",
    "        raise NotImplemented(\"You should implement this method\")\n",
    "\n",
    "    def search(self, query:str, k:int=5) -> List[str]:\n",
    "        \"\"\"Use the BM25 retriever to find the k documents that are closest\n",
    "        to the provided query\"\"\"\n",
    "\n",
    "        raise NotImplemented(\"You should implement this method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rank-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "from typing import List\n",
    "\n",
    "class BM25Retriever:\n",
    "\n",
    "    def __init__(self, filename=\"/fp/projects01/ec403/IN4080/starwars.json\"):\n",
    "        \"\"\"Using the JSON file provided as input, create a BM25s retriever \n",
    "        containing all (indexed) documents.\"\"\"\n",
    "        \n",
    "        with open(filename, 'r') as file:\n",
    "            self.documents = json.load(file)\n",
    "\n",
    "        self.texts = list(self.documents.values())\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.tokenized_documents = [self.tokenize(doc) for doc in self.texts]\n",
    "        self.bm25 = BM25Okapi(self.tokenized_documents)\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenize the input text and filter out stop words.\"\"\"\n",
    "        \n",
    "        tokens = text.lower().split()\n",
    "        filtered_tokens = [token for token in tokens if token not in self.stop_words]\n",
    "        return filtered_tokens\n",
    "\n",
    "    def search(self, query: str, k: int = 5) -> List[str]:\n",
    "        \"\"\"Use the BM25 retriever to find the k documents that are closest\n",
    "        to the provided query.\"\"\"\n",
    "        \n",
    "        tokenized_query = self.tokenize(query)\n",
    "        scores = self.bm25.get_scores(tokenized_query)\n",
    "        top_k_indices = scores.argsort()[-k:][::-1]\n",
    "\n",
    "        return [self.texts[i] for i in top_k_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then test our retriever by checking whether the documents with highest BM25 scores are indeed the ones that are most relevant to the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who is Luke Skywalker?\n",
      "Retrieved documents:\n",
      "- The mecho-organic droid was a drone created by Ship to wield a lightsaber in combat against Luke Skywalker. At first, Ship thought Luke was merely another simulated opponent, part of the games he played with the ship's computer. When Luke managed to overcome the mecho-organic droid, however, Ship realized that Luke was real.\n",
      "- Luke Skywalker's lightsaber was the first lightsaber constructed by Luke Skywalker and the second one he owned.\n",
      "- Yoda Teaches Driver's Ed is a comic printed in Star Wars Jedi Quest Kids Club 3. Luke Skywalker asks Yoda if Yoda is certain Luke needs a driver's license to be a Jedi. Yoda chastises Luke not to question his teaching. Han Solo asks Luke if he wants to \"drag\" before saying \"Wrong movie!\" and \"Outta my way graffiti geek!\" (referencing Harrison Ford's character in American Graffiti). Han speeds off, leaving Luke coughing in his exhaust. Darth Vader implores Luke to complete his training so the two of them can \"cruise the galaxy together,\" but Luke complains that he's trying but the vehicle he's in won't move because it's made of plastic. Then Luke uses the force to make the plastic speeder move. Vader says Luke's destiny is with him, and Yoda notes with approval that Luke at least remembered to buckle his seatbelt.\n",
      "- The Luke Skywalker X-Wing Mech was a mech piloted by Luke Skywalker. It shared a design with his X-wing and included a large lightsaber.\n",
      "- Farnay was a female Devaronian and the daughter of Kivas. Shortly after the Battle of Yavin, Kivas repaired the Y 4 BTL-S3 Y-wing Starfighter belonging to Luke Skywalker, a rebel who crash-landed near Tikaroo. She wanted to guide Luke to the Temple of Eedit, but Sarco Plank was more convincing. She warned Luke that Sarco was a ghoul and followed them secretly. A few days later an Imperial Sentinel-class landing craft landed in the city with eight Imperial stormtroopers and a major. They inquired of a rebel and his Y-wing and when Farnay's father told the truth they captured her and went to the Temple of Eedit. Once there Sarco and Luke killed all the Imperials and rescued her. However, Sarco wanted to kill Luke as well. She helped Luke to defeat Sarco and later they went back to Tikaroo. After the Battle of Endor, Luke Skywalker returned to Devaron to see Farnay. C-3PO mentioned that she was a \"capable young woman\" by this point.\n",
      "===========\n",
      "Question: Where is the Niima Outpost in Star Wars?\n",
      "Retrieved documents:\n",
      "- The Niima Outpost Militia was a law enforcement agency stationed at Niima Outpost on the desert planet of Jakku. It was led by the Kyuzo Constable Zuvio and his two cousins&mdash;Drego and Streehn.\n",
      "- The Office of the Constable was a small building used as a headquarters by the Niima Outpost Militia on Jakku.\n",
      "- Niima Outpost was a junkyard settlement on Jakku, a desert planet in the Western Reaches of the galaxy. The outpost was named for and founded by Niima the Hutt after the Battle of Jakku to capitalize on the new scavenging opportunities the battle created on the planet. Niima Outpost was the only spaceport on the planet, although it was referred more as a landing field rather than a spaceport.Rey's Survival Guide Scavengers, like Rey, salvaged materials from the technology leftover from the Battle of Jakku. Salvage made up the backbone of the economy, but other branches such as black market trading, mercenaries for hire, and other unlawful activities exist.  Niima's fuel supplies were basic, though there were some scavengers who located military grade supplies, such as rhydonium. Niima attracted a lot of attention offworld because of it being the only navigational beacon on the planet. These foreigners were encouraged to lock up their ships. Most travelers opted to sleep on their ships due to lack of permanent structures in the outpost. Scavengers brought their finds to Niima Outpost in exchange for supplies from Unkar Plutt, the local boss who operated from the center of awning-roofed blockhouse known as the Concession Stand. Niima Outpost was also the location of Constable Zuvio's office. The outpost's main gate utilized architecture akin to that found on other Hutt worlds such as Teth. Sarco Plank opened a gun shop at the outpost to modify and enhance blasters.  The outpost was attacked by the First Order, who were looking for the astromech droid BB-8 and stormtrooper defector Finn.\n",
      "- Bay Three was a docking bay in Niima Outpost on the planet Jakku. When the scavenger Rey took BB-8 to Niima Outpost, she told the droid there was a trader in Bay Three named Horvins who may have been willing to give BB-8 a lift offworld.\n",
      "- Taryish Juhden was a human male who spent time in Niima Outpost on the planet Jakku around thirty years after the Battle of Endor. While at the outpost he encountered the scavenger Rey several times, and watched her fend off two thugs working for Unkar Plutt as they tried to steal the astromech droid BB-8. He later fled when Niima was attacked by the forces of the First Order, running past stormtroopers firing on civilians.\n",
      "===========\n",
      "Question: Have you heard of Nute Gunray? Who is he?\n",
      "Retrieved documents:\n",
      "- Nute Gunray's citadel, also referred to as Nute Gunray's redoubt, was a large fortress located on the western hemisphere of Cato Neimoidia. It was used as a stronghold and storehouse by Nute Gunray, Viceroy of the Trade Federation.\n",
      "- The Viceroy's collar was an item worn by the Viceroy of the Trade Federation. Nute Gunray wore the collar.\n",
      "- The sovereign beetle was a species from Neimoidia. The patterns on its shell were the basis of the ornamentation on Nute Gunray's mechno-chair.Cloak of Deception\n",
      "- t'laalak-s'lalak-t'th'ak was the Xi Charrian who designed the holographic-projector chair that Nute Gunray used to communicate with Darth Sidious.\n",
      "- In 44 BBY, then-Senator Nute Gunray used a Trade Federation shuttle of a different class than the Sheathipede-class transport shuttle.\n",
      "===========\n",
      "Question: What kind of planet is Kashyyyk, and who discovered it?\n",
      "Retrieved documents:\n",
      "- A rare and unique spice variant was discovered during the Galactic Civil War. This kind of spice appeared organicQuest: \"Man Down!\" and medicinal in nature.\n",
      "- Munni was a kind of graceful kelp from the planet Naboo.\n",
      "- Tanaabian straw was a kind of plant from the planet Taanab.\n",
      "- A tisane was a kind of brewed beverage. Shig was a kind of Mandalorian tisane.\n",
      "- Puk was a kind of plant grown in gardens on the planet Tatooine.\n",
      "===========\n",
      "Question: Who are Condlurans, and can you give 2-3 names of known Condlurans?\n",
      "Retrieved documents:\n",
      "- Episode 2-3 is the third episode of the second season of the web-TV series Andor. The episode was written by showrunner Tony Gilroy.\n",
      "- A core name was a shortened name used by Chiss. Members of the Chiss species used their core names rather than their full names for at least two reasons. Among Chiss, core names were used in all but the most formal settings. Chiss also gave their core names to members of other species, as non-Chiss had difficulties pronouncing full Chiss names.\n",
      "- Redirects from alternate names\n",
      "- Redirects from alternate names\n",
      "- Redirects from alternate names\n",
      "===========\n",
      "Question: What can you tell me about the First Battle of Geonosis?\n",
      "Retrieved documents:\n",
      "- Mila was a human fortune-teller who lived at Takodana Castle on the planet Takodana by the time of the First Order-Resistance War. Mila used the ancient, historically forbidden Bakurat card deck to tell fortunes.\n",
      "- Star Wars: Battle of Jakku &mdash; Insurgency Rising is an upcoming canon comic book limited series, and the first of three limited series known as Star Wars: Battle of Jakku. It is set between Episode VI Return of the Jedi and Episode VII The Force Awakens and will tell the story of the Galactic Civil War after the Battle of Endor leading up to the Battle of Jakku. It is written by Alex Segura and illustrated by Leonard Kirk , Stefano Raffaele and Jethro Morales and will be published by Marvel Comics between October 2 and November 6, 2024.\n",
      "- Range Lead was the call sign General Dendo used during the Battle of Ithor. He used it to tell Corran Horn that Gavin Darklighter had determined which transport was being used as the Yuuzhan Vong's command center on Ithor.\n",
      "- Star Wars: Battle of Jakku also known as Star Wars: The Battle of Jakku is an upcoming series of canon comic book limited series to be written by Alex Segura. It is set between Episode VI Return of the Jedi and Episode VII The Force Awakens and will tell the story of the Galactic Civil War after the Battle of Endor leading up to the Battle of Jakku.\n",
      "- Lupre'en was a language that was spoken in the galaxy. The Lupr'or spoke a variation on Lupre'en which Captain Phasma of the First Order was able to understand. The BB unit BB-K8 once confused the Lupre'en word for \"everyone\" with \"men\", causing TN-3465 to tell her local guide Dar'en to bring the men when she meant everyone in the village.\n",
      "===========\n",
      "Question: What is the name of the settlement where Anakin Skywalker and his mother lived?\n",
      "Retrieved documents:\n",
      "- The Zephyr-G swoop was a type of swoop bike manufactured by Mobquet Swoops and Speeders. Owen Lars owned one, which Anakin Skywalker used during his search for his mother Shmi Skywalker Lars.\n",
      "- Finn's mother lived with him on Coruscant. In 32 BBY, she helped Anakin Skywalker return to the Jedi Temple after he helped her son repair their malfunctioning nanny droid.\n",
      "- Mos Espa was a spaceport settlement located on the desert world of Tatooine. The settlement included a number of commercial and workspace settings, as well as entertainment establishments such as the Mos Espa Grand Arena. During the Invasion of Naboo, Mos Espa was also home to a number of slaves, including Anakin Skywalker and his mother, Shmi.\n",
      "- Voteb was a remote settlement that was located on the Outer Rim planet Lanteeb. During the Clone Wars, the Jedi Obi-Wan Kenobi and Anakin Skywalker ventured to Lanteeb on a mission to counter the development of a Separatist bioweapon, and while on the world the pair disguised themselves as natives from Voteb.\n",
      "- The Skywalker home was where the slaves Anakin Skywalker and his mother Shmi Skywalker Lars lived in Mos Espa on the desert planet Tatooine. By the time the handmaiden Sabé traveled to Tatooine on behalf of Padmé Amidala to free Shmi from slavery, Shmi no longer lived in the home and the door had a newly cut symbol of a white sun on it.Queen's Shadow\n",
      "===========\n",
      "Question: Which planet did Darth Sidious represent as senator?\n",
      "Retrieved documents:\n",
      "- Vic Vankoh was a human male musician who once performed on the planet Scarif for an audience that included the Sith Lords Darth Sidious and Darth Vader.\n",
      "- Hitaka was a farming planet located in the Cademimu sector of the Outer Rim Territories, with a lush landscape of forests and mountains.  During the Clone Wars, Darth Tyranus was sent by the Dark Lord of the Sith Darth Sidious to Hitaka to complete a task. Sidious then arranged for the Grand Army of the Republic to engage in a battle on the planet to make it easier for his apprentice to move about unnoticed.\n",
      "- The Order of Shasa was a Force-sensitive cult of Selkath natives of planet Manaan who were duped to serve Darth Sidious during the tenure of his Galactic Empire.\n",
      "- Darth was a title that preceded the moniker of a Sith Lord. Predating the history of the Rule of Two, it was used by ancient Sith such as Darth Atrius, Darth Caldoth, and Darth Noctyss. The Darth title was preserved through Darth Bane's reforms, spawning a thousand-year lineage that included Darth Zannah, Darth Tenebrous, and Darth Plagueis. Darth Sidious was the culmination of generations of Sith leading to the rise of the Galactic Empire. As a Sith Master, Sidious bestowed the title of Darth on his three successive Sith apprentices: Darth Maul, Darth Tyranus, and Darth Vader.\n",
      "- This blue-bladed lightsaber belonged to the Sith Master Darth Sidious after his resurrection on Byss.\n",
      "===========\n"
     ]
    }
   ],
   "source": [
    "retriever = BM25Retriever()\n",
    "for question in questions:\n",
    "    print(\"Question:\", question)\n",
    "    print(\"Retrieved documents:\")\n",
    "    for relevant_doc in retriever.search(question):\n",
    "        print(\"- \" + relevant_doc.replace(\"\\n\", \" \"))\n",
    "    print(\"===========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation is correct, the retrieved documents should for the most part relevant to the query. \n",
    "\n",
    "### Dense retrieval \n",
    "\n",
    "Many of those documents are, however, way too long to be included in the prompt for our Gemma model (especially if we wish to include 4-5 retrieved texts for each query!). Can we ensure that the length of each retrieved text stays within a reasonable length, such as one or two sentences? \n",
    "\n",
    "One strategy is to not return the full documents, but instead determine the most relevant _sentences_ within those documents. But how do we determine which sentence is most relevant? A sparse retriever using BM25 would not work well here, as it does not really account for the semantics of the query. Instead, what we can do is to:\n",
    "- split the documents (retrieved through BM25) into sentences\n",
    "- extract sentence embeddings for the query and for each sentence\n",
    "- compute the cosine similarities between the query vector and each sentence vector\n",
    "- and return the _k_ most similar sentences\n",
    "\n",
    "In other words, our approach starts with a _sparse retrieval step_ at the level of full documents (which we already have implemented, using BM25S), and continues with a _dense retrieval step_ to determine the most relevant sentences among the sentences that are found in the retrieved documents.\n",
    "\n",
    "**Task 3** (4 points): Re-implement the `search` method to segment into sentences each document retrieved with BM25, extract sentence embeddings for the query and sentences using the encoder model (see [here](https://sbert.net/examples/applications/semantic-search/README.html) for explanations and code examples), and then select the _k_ sentences with highest cosine similarities.  \n",
    "\n",
    "_Tips_: You can use `nltk.sent_tokenize` to segment your document in sentences."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import bm25s\n",
    "import re, json\n",
    "import sentence_transformers\n",
    "import nltk\n",
    "#nltk.download('punkt_tab')\n",
    "from typing import List\n",
    "\n",
    "class HybridRetriever(BM25Retriever):\n",
    "\n",
    "    def __init__(self, filename=\"/fp/projects01/ec403/IN4080/starwars.json\", \n",
    "                 encoder_model=\"msmarco-MiniLM-L-6-v3\"):\n",
    "        \n",
    "        \"\"\"Using the json file provided as input, create a BM25s retriever \n",
    "        containing all (indexed) documents, and loads a sentence transformer model\n",
    "        used to compute the embeddings for the query and sentences\"\"\"\n",
    "\n",
    "        BM25Retriever.__init__(self, filename)\n",
    "        self.encoder = sentence_transformers.SentenceTransformer(encoder_model)\n",
    "\n",
    "    def search(self, query:str, k:int=5) -> List[str]:\n",
    "        \"\"\"Use the BM25 retriever to find the documents that are closest\n",
    "        to the provided query, and then the sentence transformer model to\n",
    "        determine the most relevant sentences\"\"\"\n",
    "\n",
    "        docs = BM25Retriever.search(self, query, k)\n",
    "\n",
    "        raise NotImplemented(\"You should implement this method\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bm25s\n",
    "import re, json\n",
    "import sentence_transformers\n",
    "import nltk\n",
    "from typing import List\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class Retriever_mk2(BM25Retriever): # we add sentence embeddings\n",
    "\n",
    "    def __init__(self, filename=\"/fp/projects01/ec403/IN4080/starwars.json\", \n",
    "                 encoder_model=\"msmarco-MiniLM-L-6-v3\"):\n",
    "        \n",
    "        \"\"\"Using the json file provided as input, create a BM25 retriever \n",
    "        containing all (indexed) documents, and loads a sentence transformer model\n",
    "        used to compute the embeddings for the query and sentences\"\"\"\n",
    "\n",
    "        BM25Retriever.__init__(self, filename)\n",
    "        self.encoder = sentence_transformers.SentenceTransformer(encoder_model) # transformer for embeddings\n",
    "\n",
    "    def search(self, query: str, k: int = 5) -> List[str]:\n",
    "        \"\"\"Use the BM25 retriever to find the documents that are closest\n",
    "        to the provided query, and then the sentence transformer model to\n",
    "        determine the most relevant sentences\"\"\"\n",
    "\n",
    "        docs = BM25Retriever.search(self, query, k) # top-k documents\n",
    "        all_sentences = []\n",
    "        for doc in docs:\n",
    "            sentences = nltk.sent_tokenize(doc)  # segment into sentences\n",
    "            all_sentences.extend(sentences) # ìsentences from top-k documents\n",
    "\n",
    "        # Step 3: Compute sentence embeddings for the query and all sentences\n",
    "        query_embedding = self.encoder.encode(query)  # query embedding\n",
    "        sentence_embeddings = self.encoder.encode(all_sentences)  # embeddings for all sentences\n",
    "\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        similarities = cosine_similarity([query_embedding], sentence_embeddings)[0] # compute distance\n",
    "        top_k_indices = similarities.argsort()[-k:][::-1]  # top-k sentences with the highest similarities\n",
    "        \n",
    "        return [all_sentences[i] for i in top_k_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can test our hybrid (sparse followed by dense) retriever on the same questions as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who is Luke Skywalker?\n",
      "Retrieved documents:\n",
      "- Luke Skywalker's lightsaber was the first lightsaber constructed by Luke Skywalker and the second one he owned.\n",
      "- The Luke Skywalker X-Wing Mech was a mech piloted by Luke Skywalker.\n",
      "- Shortly after the Battle of Yavin, Kivas repaired the Y 4 BTL-S3 Y-wing Starfighter belonging to Luke Skywalker, a rebel who crash-landed near Tikaroo.\n",
      "- Luke Skywalker asks Yoda if Yoda is certain Luke needs a driver's license to be a Jedi.\n",
      "- Darth Vader implores Luke to complete his training so the two of them can \"cruise the galaxy together,\" but Luke complains that he's trying but the vehicle he's in won't move because it's made of plastic.\n",
      "===========\n",
      "Question: Where is the Niima Outpost in Star Wars?\n",
      "Retrieved documents:\n",
      "- Niima Outpost was a junkyard settlement on Jakku, a desert planet in the Western Reaches of the galaxy.\n",
      "- Niima Outpost was also the location of Constable Zuvio's office.\n",
      "- Niima Outpost was the only spaceport on the planet, although it was referred more as a landing field rather than a spaceport.Rey's Survival Guide Scavengers, like Rey, salvaged materials from the technology leftover from the Battle of Jakku.\n",
      "- The Niima Outpost Militia was a law enforcement agency stationed at Niima Outpost on the desert planet of Jakku.\n",
      "- Bay Three was a docking bay in Niima Outpost on the planet Jakku.\n",
      "===========\n",
      "Question: Have you heard of Nute Gunray? Who is he?\n",
      "Retrieved documents:\n",
      "- Nute Gunray wore the collar.\n",
      "- In 44 BBY, then-Senator Nute Gunray used a Trade Federation shuttle of a different class than the Sheathipede-class transport shuttle.\n",
      "- Nute Gunray's citadel, also referred to as Nute Gunray's redoubt, was a large fortress located on the western hemisphere of Cato Neimoidia.\n",
      "- It was used as a stronghold and storehouse by Nute Gunray, Viceroy of the Trade Federation.\n",
      "- t'laalak-s'lalak-t'th'ak was the Xi Charrian who designed the holographic-projector chair that Nute Gunray used to communicate with Darth Sidious.\n",
      "===========\n",
      "Question: What kind of planet is Kashyyyk, and who discovered it?\n",
      "Retrieved documents:\n",
      "- Puk was a kind of plant grown in gardens on the planet Tatooine.\n",
      "- A rare and unique spice variant was discovered during the Galactic Civil War.\n",
      "- Tanaabian straw was a kind of plant from the planet Taanab.\n",
      "- and medicinal in nature.\n",
      "- This kind of spice appeared organicQuest: \"Man Down!\"\n",
      "===========\n",
      "Question: Who are Condlurans, and can you give 2-3 names of known Condlurans?\n",
      "Retrieved documents:\n",
      "- Among Chiss, core names were used in all but the most formal settings.\n",
      "- A core name was a shortened name used by Chiss.\n",
      "- Chiss also gave their core names to members of other species, as non-Chiss had difficulties pronouncing full Chiss names.\n",
      "- Members of the Chiss species used their core names rather than their full names for at least two reasons.\n",
      "- Redirects from alternate names\n",
      "===========\n",
      "Question: What can you tell me about the First Battle of Geonosis?\n",
      "Retrieved documents:\n",
      "- He used it to tell Corran Horn that Gavin Darklighter had determined which transport was being used as the Yuuzhan Vong's command center on Ithor.\n",
      "- Range Lead was the call sign General Dendo used during the Battle of Ithor.\n",
      "- Star Wars: Battle of Jakku &mdash; Insurgency Rising is an upcoming canon comic book limited series, and the first of three limited series known as Star Wars: Battle of Jakku.\n",
      "- Star Wars: Battle of Jakku also known as Star Wars: The Battle of Jakku is an upcoming series of canon comic book limited series to be written by Alex Segura.\n",
      "- It is set between Episode VI Return of the Jedi and Episode VII The Force Awakens and will tell the story of the Galactic Civil War after the Battle of Endor leading up to the Battle of Jakku.\n",
      "===========\n",
      "Question: What is the name of the settlement where Anakin Skywalker and his mother lived?\n",
      "Retrieved documents:\n",
      "- The Skywalker home was where the slaves Anakin Skywalker and his mother Shmi Skywalker Lars lived in Mos Espa on the desert planet Tatooine.\n",
      "- During the Invasion of Naboo, Mos Espa was also home to a number of slaves, including Anakin Skywalker and his mother, Shmi.\n",
      "- Owen Lars owned one, which Anakin Skywalker used during his search for his mother Shmi Skywalker Lars.\n",
      "- In 32 BBY, she helped Anakin Skywalker return to the Jedi Temple after he helped her son repair their malfunctioning nanny droid.\n",
      "- Finn's mother lived with him on Coruscant.\n",
      "===========\n",
      "Question: Which planet did Darth Sidious represent as senator?\n",
      "Retrieved documents:\n",
      "- Darth Sidious was the culmination of generations of Sith leading to the rise of the Galactic Empire.\n",
      "- This blue-bladed lightsaber belonged to the Sith Master Darth Sidious after his resurrection on Byss.\n",
      "- Sidious then arranged for the Grand Army of the Republic to engage in a battle on the planet to make it easier for his apprentice to move about unnoticed.\n",
      "- As a Sith Master, Sidious bestowed the title of Darth on his three successive Sith apprentices: Darth Maul, Darth Tyranus, and Darth Vader.\n",
      "- Darth was a title that preceded the moniker of a Sith Lord.\n",
      "===========\n"
     ]
    }
   ],
   "source": [
    "\n",
    "retriever = Retriever_mk2()\n",
    "for question in questions:\n",
    "    print(\"Question:\", question)\n",
    "    print(\"Retrieved documents:\")\n",
    "    for relevant_doc in retriever.search(question):\n",
    "        print(\"- \" + relevant_doc.replace(\"\\n\", \" \"))\n",
    "    print(\"===========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Now that we have a functioning retriever model, we can connect it to the generative language model employed to produce the responses.\n",
    "\n",
    "**Task 4** (4 points): Implement the `RetrievalAugmentedResponseGenerator`. Given an initial input prompt, the method should first retrieve relevant sentences using the `HybridRetriever` we have just developed. Then, it should expand the initial prompt using the provided template (you are of course free to edit or adapt it as you see fit). This expanded prompt should then be tokenized and fed as input to the LLM in the same way as before."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "PROMPT_TEMPLATE = \"'You are given the following information about Star Wars:\\n-{retrieved_sentences}\\nNow answer the following question in 1 or 2 sentences, based on the provided information: '{query}'\"\n",
    "\n",
    "class RetrievalAugmentedResponseGenerator:\n",
    "\n",
    "    def __init__(self, model_name=\"google/gemma-1.1-2b-it\", \n",
    "                 doc_filename=\"/fp/projects01/ec403/IN4080/starwars.json\", \n",
    "                 encoder_model=\"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"Loads the tokenizer, pretrained causal LM for the given model, along with the \n",
    "        hybrid sparse-dense retriever model populated with the documents in doc_filename.\"\"\"\n",
    "\n",
    "        raise NotImplemented(\"You must implement this method\")\n",
    "\n",
    "    def get_response(self, query:str, max_length:int=50, k=3) -> str:\n",
    "        \"\"\"Given a prompt, retrieve k relevant sentences, generate a response (of a maximum \n",
    "        max_length tokens) and return it.\n",
    "        Only the response should be returned, not the text of the prompt itself\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplemented(\"You must implement this method\")\n",
    "\n",
    "\n",
    "agent = RetrievalAugmentedResponseGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f844c5165df43f08122a8229ebe8dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "PROMPT_TEMPLATE = (\n",
    "    \"You are given the following information about Star Wars:\\n-{retrieved_sentences}\\n\"\n",
    "    \"Now answer the following question in 1 or 2 sentences, based on the provided information: '{query}'\"\n",
    ")\n",
    "\n",
    "class RetrievalAugmentedResponseGenerator:\n",
    "\n",
    "    def __init__(self, model_name=\"google/gemma-1.1-2b-it\", \n",
    "                 doc_filename=\"/fp/projects01/ec403/IN4080/starwars.json\", \n",
    "                 encoder_model=\"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"Loads the tokenizer, pretrained causal LM for the given model, along with the \n",
    "        hybrid sparse-dense retriever model populated with the documents in doc_filename.\"\"\"\n",
    "        \n",
    "        # Load the Retriever_mk2\n",
    "        self.retriever = Retriever_mk2(filename=doc_filename, encoder_model=encoder_model)\n",
    "\n",
    "        # Load the language model and tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    def get_response(self, query: str, max_length: int = 200, k: int = 3) -> str:\n",
    "        \"\"\"Given a prompt, retrieve k relevant sentences, generate a response (of a maximum \n",
    "        max_length tokens) and return it.\n",
    "        Only the response should be returned, not the text of the prompt itself.\n",
    "        \"\"\"\n",
    "        \n",
    "        retrieved_sentences = self.retriever.search(query, k=k)\n",
    "        retrieved_text = \"\\n-\".join(retrieved_sentences)  # Format sentences\n",
    "        formatted_prompt = PROMPT_TEMPLATE.format(retrieved_sentences=retrieved_text, query=query) # prompt using PROMPT_TEMPLATE\n",
    "        input_ids = self.tokenizer.encode(formatted_prompt, return_tensors='pt')\n",
    "        output_ids = self.model.generate(input_ids, max_length=max_length, num_return_sequences=1, pad_token_id=self.tokenizer.eos_token_id) # Generating response\n",
    "        response = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        response_text = response[len(formatted_prompt):].strip() # extract the response\n",
    "\n",
    "        return response_text\n",
    "\n",
    "agent = RetrievalAugmentedResponseGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to test our system end-to-end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who is Luke Skywalker?\n",
      "Answer: Based on the provided information, Luke Skywalker is a young man who is the second owner of a lightsaber and has a questionable relationship with Darth Vader.\n",
      "-------\n",
      "Question: Where is the Niima Outpost in Star Wars?\n",
      "Answer: The Niima Outpost is located on Jakku, a desert planet in the Western Reaches of the galaxy.\n",
      "-------\n",
      "Question: Have you heard of Nute Gunray? Who is he?\n",
      "Answer: Nute Gunray is a character from the Star Wars universe. He is known for wearing the collar and being the Viceroy of the Trade Federation.\n",
      "-------\n",
      "Question: What kind of planet is Kashyyyk, and who discovered it?\n",
      "Answer: We do not have enough information to determine the planet of Kashyyyk or who discovered it from the provided text.\n",
      "-------\n",
      "Question: Who are Condlurans, and can you give 2-3 names of known Condlurans?\n",
      "Answer: The provided text does not contain any information regarding Condlurans, so I am unable to answer this question from the provided context.\n",
      "-------\n",
      "Question: What can you tell me about the First Battle of Geonosis?\n",
      "Answer: I am unable to extract the requested information from the provided context, so I am unable to answer this question.\n",
      "-------\n",
      "Question: What is the name of the settlement where Anakin Skywalker and his mother lived?\n",
      "Answer: The provided text does not contain any information regarding the name of the settlement where Anakin Skywalker and his mother lived, so I am unable to answer this question from the provided context.\n",
      "-------\n",
      "Question: Which planet did Darth Sidious represent as senator?\n",
      "Answer: The provided text does not contain any information regarding which planet Darth Sidious represented as senator, so I am unable to answer this question from the provided context.\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "for question in questions:\n",
    "    print(\"Question:\", question)\n",
    "    print(\"Answer:\", agent.get_response(question))\n",
    "    print(\"-------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5** (4 points): If you have implemented your model correctly, the system should answer correctly to at least a few questions. But it is still far from perfect, and some of the answers are flat-out wrong. Suggest 2-3 ways one could improve the current system and get even better answers. You don't need to implement anything, simply flesh out a few ideas you believe are worth trying out.\n",
    "\n",
    "_(of course, it is even better if you actually try to implement those ideas and evaluate their influence on the quality of the system responses!)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "To improve the current system, that is basically Retrieval-Augmented Generation (RAG), we can implement new techniques in the retrieval and generation components. For example we can implement the Improving Contextual Relevance in Retrieval. Currently, the 'Retriever_mk2' is only using BM25 and basic sentence embeddings to rank sentences. We can enhance this by using more advanced dense retrievers that are fine-tuned on a relevant dataset (e.g., MS MARCO or other QA-specific datasets). Hybrid sparse-dense retrieval approaches like BM25 + Sentence Embeddings can sometimes miss semantic nuances. A dense retriever fine-tuned for question answering can better capture the semantics of the query.\n",
    "\n",
    "We can also try the Answer Consistency through Context Re-Ranking, because instead of relying solely on sentence embeddings for relevance, re-rank the retrieved sentences using a cross-encoder model like cross-encoder/ms-marco-MiniLM-L-6-v2. Cross-encoders perform better than bi-encoders in terms of ranking since they consider pairwise interactions between the query and each candidate sentence.\n",
    "\n",
    "**References**\n",
    "- [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906)\n",
    "- [ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction](https://arxiv.org/abs/2112.01488)\n",
    "- [gemma-1.1-2b-it ](https://huggingface.co/google/gemma-1.1-2b-it)\n",
    "- [A block-sparse Tensor Train Format for sample-efficient high-dimensional Polynomial Regression](https://arxiv.org/abs/2104.14255)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
